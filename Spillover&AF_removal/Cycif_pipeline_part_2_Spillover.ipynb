{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c869addf",
   "metadata": {},
   "source": [
    "# Spillover Removal â€“ Marker CSV (normalisierte Pipeline wie V5)\n",
    "\n",
    "Diese Version repliziert das Verhalten des klassischen Mosaic/Spillover-UI: \n",
    "- Komplett-Stack wird pro Kanal auf 0â€“1 normalisiert.\n",
    "- Spillover-Koeffizienten werden via Mutual Information geschÃ¤tzt und auf den normalisierten Stack angewandt.\n",
    "- Danach wird jeder Kanal wieder in den ursprÃ¼nglichen Wertebereich zurÃ¼cktransformiert und im Original-Datentyp gespeichert.\n",
    "\n",
    "Damit sollte die Spillover-Entfernung den bekannten Effekt zeigen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23fdf7",
   "metadata": {},
   "source": [
    "## 1. Sample Configuration (Dynamisch wie Script 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071b0cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ SAMPLE: sample_197\n",
      "ðŸ“‚ WORKSPACE_ROOT: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\n",
      "ðŸ“‚ BASE_EXPORT_ROOT: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\n",
      "ðŸ“‚ BASE_EXPORT (Sample): C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\n",
      "âœ… Sample-Verzeichnis gefunden\n",
      "âœ… Input-TIF gefunden: multicycle_mosaics\\decon2D\\decon2D_fused\\fused_decon.tif\n",
      "Konfiguration geladen. Input: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\multicycle_mosaics\\decon2D\\decon2D_fused\\fused_decon.tif\n",
      "Marker CSV: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\Markers_197.csv\n"
     ]
    }
   ],
   "source": [
    "# === ðŸŽ¯ SAMPLE CONFIGURATION (DYNAMISCH wie Script 1) ===\n",
    "from pathlib import Path\n",
    "\n",
    "# === 1. SAMPLE-AUSWAHL ===\n",
    "current_sample = \"sample_197\"  # â† HIER SAMPLE Ã„NDERN (muss mit Script 1 Ã¼bereinstimmen)\n",
    "\n",
    "print(f\"ðŸŽ¯ SAMPLE: {current_sample}\")\n",
    "\n",
    "# === 2. DYNAMISCHE PFAD-STRUKTUR (wie Script 1) ===\n",
    "# Pfade werden automatisch aus current_sample abgeleitet\n",
    "WORKSPACE_ROOT = Path(r\"C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\")\n",
    "BASE_EXPORT_ROOT = WORKSPACE_ROOT / \"data\" / \"export\"\n",
    "BASE_EXPORT = BASE_EXPORT_ROOT / current_sample\n",
    "\n",
    "# Legacy-KompatibilitÃ¤t\n",
    "WORKSPACE_EXPORT_ROOT = BASE_EXPORT_ROOT  \n",
    "SAMPLE_ID = current_sample\n",
    "\n",
    "print(f\"ðŸ“‚ WORKSPACE_ROOT: {WORKSPACE_ROOT}\")\n",
    "print(f\"ðŸ“‚ BASE_EXPORT_ROOT: {BASE_EXPORT_ROOT}\")  \n",
    "print(f\"ðŸ“‚ BASE_EXPORT (Sample): {BASE_EXPORT}\")\n",
    "\n",
    "# === 3. VALIDIERUNG ===\n",
    "if not BASE_EXPORT.exists():\n",
    "    raise RuntimeError(f\"âŒ FEHLER: Sample-Verzeichnis existiert nicht: {BASE_EXPORT}\")\n",
    "else:\n",
    "    print(f\"âœ… Sample-Verzeichnis gefunden\")\n",
    "\n",
    "# === 4. GLOBALE VARIABLEN SETZEN ===\n",
    "globals()[\"WORKSPACE_ROOT\"] = WORKSPACE_ROOT\n",
    "globals()[\"WORKSPACE_EXPORT_ROOT\"] = WORKSPACE_EXPORT_ROOT\n",
    "globals()[\"BASE_EXPORT_ROOT\"] = BASE_EXPORT_ROOT\n",
    "globals()[\"BASE_EXPORT\"] = BASE_EXPORT\n",
    "globals()[\"current_sample\"] = current_sample\n",
    "globals()[\"SAMPLE_ID\"] = SAMPLE_ID\n",
    "\n",
    "# === INPUT-FILE FINDER (STRENG: Nur aus multicycle_mosaics/decon2D_fused/) ===\n",
    "MULTICYCLE_DIR = BASE_EXPORT / \"multicycle_mosaics\"\n",
    "DECON_DIR = MULTICYCLE_DIR / \"decon2D\"\n",
    "FUSED_DIR = DECON_DIR / \"decon2D_fused\"\n",
    "\n",
    "if not FUSED_DIR.exists():\n",
    "    raise RuntimeError(\n",
    "        f\"âŒ FEHLER: FUSED_DIR nicht gefunden: {FUSED_DIR}\\n\"\n",
    "        f\"   Erwartete Struktur: {BASE_EXPORT}/multicycle_mosaics/decon2D/decon2D_fused/\"\n",
    "    )\n",
    "\n",
    "# Suche EXPLICIT in FUSED_DIR (NICHT rekursiv!)\n",
    "input_candidates = sorted(FUSED_DIR.glob(\"fused_decon*.tif\"))  # Auch .tif ohne .ome akzeptieren\n",
    "if not input_candidates:\n",
    "    raise FileNotFoundError(\n",
    "        f\"âŒ FEHLER: Kein fused_decon*.tif in {FUSED_DIR} gefunden.\\n\"\n",
    "        f\"   Script 2 (Spillover) erwartet Input aus Part 1 (Stitching/Decon/EDF).\\n\"\n",
    "        f\"   Bitte Part 1 ausfÃ¼hren BEVOR Part 2 gestartet wird.\"\n",
    "    )\n",
    "if len(input_candidates) > 1:\n",
    "    raise RuntimeError(\n",
    "        f\"âŒ FEHLER: Mehrere fused_decon*.ome.tif gefunden in {FUSED_DIR}:\\n\"\n",
    "        + '\\n'.join(f\"   - {p.name}\" for p in input_candidates)\n",
    "        + \"\\n   Bitte unerwÃ¼nschte Dateien entfernen oder umbenennen.\"\n",
    "    )\n",
    "INPUT_PATH = input_candidates[0]\n",
    "print(f\"âœ… Input-TIF gefunden: {INPUT_PATH.relative_to(BASE_EXPORT)}\")\n",
    "\n",
    "sample_token = ''.join(ch for ch in SAMPLE_ID if ch.isdigit()) or SAMPLE_ID\n",
    "marker_candidates = [\n",
    "    csv_path for csv_path in BASE_EXPORT.glob('markers_*.csv')\n",
    "    if sample_token.lower() in csv_path.stem.lower()\n",
    "]\n",
    "if not marker_candidates:\n",
    "    raise FileNotFoundError(\n",
    "        f'Erwartete Marker-CSV markers_{sample_token}.csv nicht in {BASE_EXPORT} gefunden.'\n",
    "    )\n",
    "if len(marker_candidates) > 1:\n",
    "    raise RuntimeError(\n",
    "        'Mehrere Marker-CSV-Dateien gefunden. Bitte unerwÃ¼nschte Dateien entfernen: '\n",
    "        + ', '.join(str(p.name) for p in marker_candidates)\n",
    "    )\n",
    "MARKER_CSV = marker_candidates[0]\n",
    "\n",
    "ROI_CFG = dict(x=1696, y=4528, size=384)  # hart definierte ROI wie in V7\n",
    "ROI_PAD = 32  # zusÃ¤tzlicher Kontext fÃ¼r Koeffizienten-SchÃ¤tzung\n",
    "PERCENTILE_Q = 99.5  # Begrenzung Ã¼ber hohe Percentile (dynamischer Cap)\n",
    "SAFETY_FACTOR = 1.2  # Sicherheitsfaktor fÃ¼r den Percentile-Cap\n",
    "MAX_COEFF = 3.0  # hartes globales Maximum als Sicherung\n",
    "\n",
    "SKIP_CHANNELS = [1, 2, 3]  # 1-basiert\n",
    "RNG_SEED = 42\n",
    "\n",
    "print('Konfiguration geladen. Input:', INPUT_PATH)\n",
    "print('Marker CSV:', MARKER_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1877120",
   "metadata": {},
   "source": [
    "## 2. Imports, Logging, Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163ab23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:09:54 | INFO     | Logging initialisiert.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(PCG64) at 0x1495AC204A0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from skimage import filters, morphology\n",
    "import tifffile as tiff\n",
    "from tifffile import TiffFile\n",
    "from ipywidgets import widgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)-8s | %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    " )\n",
    "logger = logging.getLogger('SpilloverOnly')\n",
    "logger.info('Logging initialisiert.')\n",
    "\n",
    "np.random.default_rng(RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defb3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "OME_NS = {'ome': 'http://www.openmicroscopy.org/Schemas/OME/2016-06'}\n",
    "\n",
    "\n",
    "def extract_channel_names(ome_xml: str, expected: int) -> List[str]:\n",
    "    names: List[str] = []\n",
    "    if ome_xml:\n",
    "        try:\n",
    "            root = ET.fromstring(ome_xml)\n",
    "            for ch in root.findall('.//ome:Channel', OME_NS):\n",
    "                name = ch.get('Name')\n",
    "                if name:\n",
    "                    names.append(name)\n",
    "        except ET.ParseError as exc:\n",
    "            logger.warning('OME-XML konnte nicht geparst werden: %s', exc)\n",
    "    if len(names) < expected:\n",
    "        names.extend([f'C{i+1}' for i in range(len(names), expected)])\n",
    "    return names[:expected]\n",
    "\n",
    "\n",
    "def load_ome_to_chw(path: Path) -> Tuple[np.ndarray, np.dtype, List[str]]:\n",
    "    logger.info('Lade OME-TIFF: %s', path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    \n",
    "    with TiffFile(str(path)) as tf:\n",
    "        ome_xml = tf.ome_metadata\n",
    "        num_pages = len(tf.pages)\n",
    "        \n",
    "        # KRITISCH: PrÃ¼fe ob Multi-Page TIFF (Channels als separate Pages)\n",
    "        if num_pages > 1:\n",
    "            logger.info(f'Multi-Page TIFF erkannt: {num_pages} Pages (Channels)')\n",
    "            # Lade ALLE Pages als separaten Stack\n",
    "            first_page = tf.pages[0]\n",
    "            orig_dtype = first_page.dtype\n",
    "            shape = first_page.shape\n",
    "            \n",
    "            # Preallocate Stack: (Channels, Y, X)\n",
    "            arr = np.empty((num_pages, shape[0], shape[1]), dtype=orig_dtype)\n",
    "            \n",
    "            # Lade jede Page in Stack\n",
    "            logger.info(f'Lade {num_pages} Channels...')\n",
    "            for i, page in enumerate(tf.pages):\n",
    "                arr[i] = page.asarray()\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    logger.info(f'  {i + 1}/{num_pages} Channels geladen')\n",
    "            \n",
    "            logger.info(f'âœ… Alle {num_pages} Channels geladen: Shape={arr.shape}, Dtype={orig_dtype}')\n",
    "            axes = 'CYX'\n",
    "        else:\n",
    "            # Single-Page oder OME mit Achsen\n",
    "            series = tf.series[0]\n",
    "            axes = series.axes\n",
    "            arr = series.asarray()\n",
    "            orig_dtype = arr.dtype\n",
    "            \n",
    "            # Entferne T und Z (nur erster Frame/Plane)\n",
    "            for ax in ('T', 'Z'):\n",
    "                if ax in axes:\n",
    "                    idx = axes.index(ax)\n",
    "                    arr = np.take(arr, 0, axis=idx)\n",
    "                    axes = axes.replace(ax, '')\n",
    "            \n",
    "            if 'C' not in axes:\n",
    "                arr = arr[None, ...]\n",
    "                axes = 'C' + axes\n",
    "                logger.warning('Keine C-Achse gefunden â€“ nehme erste Dimension als Kanal an.')\n",
    "            \n",
    "            c_idx = axes.index('C')\n",
    "            if c_idx != 0:\n",
    "                arr = np.moveaxis(arr, c_idx, 0)\n",
    "            \n",
    "            # Reduziere auf exakt 3D (C, Y, X)\n",
    "            while arr.ndim > 3:\n",
    "                arr = arr[0]\n",
    "    \n",
    "    channel_names = extract_channel_names(ome_xml, arr.shape[0])\n",
    "    return arr.astype(np.float32), orig_dtype, channel_names\n",
    "\n",
    "\n",
    "def extract_roi_patch(img: np.ndarray, *, x: int, y: int, size: int) -> Tuple[np.ndarray, Tuple[int, int, int, int]]:\n",
    "    size = int(size)\n",
    "    half = size // 2\n",
    "    y0 = max(0, int(y) - half)\n",
    "    x0 = max(0, int(x) - half)\n",
    "    y1 = min(img.shape[-2], y0 + size)\n",
    "    x1 = min(img.shape[-1], x0 + size)\n",
    "    if (y1 - y0) < size:\n",
    "        y0 = max(0, y1 - size)\n",
    "    if (x1 - x0) < size:\n",
    "        x0 = max(0, x1 - size)\n",
    "    return img[y0:y1, x0:x1], (y0, y1, x0, x1)\n",
    "\n",
    "\n",
    "def ls_coeff(donor: np.ndarray, target: np.ndarray) -> float:\n",
    "    donor_f = donor.astype(np.float32, copy=False)\n",
    "    target_f = target.astype(np.float32, copy=False)\n",
    "    denom = float(np.sum(donor_f * donor_f) + 1e-6)\n",
    "    if denom <= 0:\n",
    "        return 0.0\n",
    "    num = float(np.sum(donor_f * target_f))\n",
    "    return max(0.0, num / denom)\n",
    "\n",
    "\n",
    "def percentile_cap(donor: np.ndarray, target: np.ndarray) -> float:\n",
    "    if PERCENTILE_Q is None:\n",
    "        return float('inf')\n",
    "    donor_q = float(np.percentile(donor, PERCENTILE_Q))\n",
    "    target_q = float(np.percentile(target, PERCENTILE_Q))\n",
    "    if not np.isfinite(donor_q) or donor_q <= 1e-6:\n",
    "        return float('inf')\n",
    "    if not np.isfinite(target_q) or target_q < 0:\n",
    "        target_q = max(0.0, target_q)\n",
    "    cap = (target_q / donor_q) * float(SAFETY_FACTOR)\n",
    "    if cap < 0:\n",
    "        return 0.0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def final_cap(raw_coeff: float, ratio_cap: float) -> float:\n",
    "    coeff = raw_coeff\n",
    "    if ratio_cap is not None:\n",
    "        coeff = min(coeff, ratio_cap)\n",
    "    if MAX_COEFF is not None:\n",
    "        coeff = min(coeff, float(MAX_COEFF))\n",
    "    return max(0.0, coeff)\n",
    "\n",
    "\n",
    "def estimate_coeff_roi(target: np.ndarray, donor: np.ndarray, *, roi_cfg: Dict[str, int], pad: int) -> Tuple[float, Dict[str, Any]]:\n",
    "    base_cfg = dict(roi_cfg)\n",
    "    roi_target, coords = extract_roi_patch(target, **base_cfg)\n",
    "    roi_donor, _ = extract_roi_patch(donor, **base_cfg)\n",
    "    if pad and pad > 0:\n",
    "        pad_cfg = dict(base_cfg)\n",
    "        pad_cfg['size'] = base_cfg['size'] + 2 * int(pad)\n",
    "        roi_target, coords = extract_roi_patch(target, **pad_cfg)\n",
    "        roi_donor, _ = extract_roi_patch(donor, **pad_cfg)\n",
    "\n",
    "    coeff_raw = ls_coeff(roi_donor, roi_target)\n",
    "    cap_ratio = percentile_cap(roi_donor, roi_target) if coeff_raw > 0 else float('inf')\n",
    "    coeff = final_cap(coeff_raw, cap_ratio)\n",
    "\n",
    "    info = dict(\n",
    "        coeff=float(coeff),\n",
    "        coeff_raw=float(coeff_raw),\n",
    "        coeff_cap_ratio=(float(cap_ratio) if np.isfinite(cap_ratio) else None),\n",
    "        pad=int(pad),\n",
    "        roi_coords=dict(top=int(coords[0]), bottom=int(coords[1]), left=int(coords[2]), right=int(coords[3])),\n",
    "        roi_sum_raw=float(np.sum(roi_target)),\n",
    "        roi_sum_donor=float(np.sum(roi_donor)),\n",
    "    )\n",
    "    return coeff, info\n",
    "\n",
    "\n",
    "def apply_spillover(stack: np.ndarray, donor_idx: int, target_idx: int, coeff: float) -> None:\n",
    "    stack[target_idx] = np.clip(stack[target_idx] - coeff * stack[donor_idx], 0, None)\n",
    "\n",
    "\n",
    "def convert_to_dtype(stack: np.ndarray, dtype: np.dtype) -> np.ndarray:\n",
    "    if np.issubdtype(dtype, np.integer):\n",
    "        info = np.iinfo(dtype)\n",
    "        out = np.clip(stack, info.min, info.max)\n",
    "    else:\n",
    "        out = stack\n",
    "    return out.astype(dtype, copy=False)\n",
    "\n",
    "\n",
    "def build_output_base(input_path: Path) -> Tuple[Path, Path, Path, Path]:\n",
    "    \"\"\"\n",
    "    Erzeugt Output-Pfade im dedizierten spillover/-Ordner.\n",
    "    \n",
    "    Struktur:\n",
    "    BASE_EXPORT/spillover/\n",
    "        â”œâ”€â”€ fused_decon_spillover_corrected.ome.tif\n",
    "        â”œâ”€â”€ spillover_coefficients.json\n",
    "        â””â”€â”€ spillover_channels/\n",
    "    \"\"\"\n",
    "    # Spillover-Ordner direkt in BASE_EXPORT (Root-bounded!)\n",
    "    spillover_dir = BASE_EXPORT / \"spillover\"\n",
    "    spillover_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Fixer Dateiname (kein Timestamp!)\n",
    "    stem = input_path.stem.replace('.ome', '')  # Entfernt .ome.tif\n",
    "    ome_path = spillover_dir / f\"{stem}_spillover_corrected.ome.tif\"\n",
    "    channel_dir = spillover_dir / \"spillover_channels\"\n",
    "    spill_path = spillover_dir / \"spillover_coefficients.json\"\n",
    "    \n",
    "    return spillover_dir, ome_path, channel_dir, spill_path\n",
    "\n",
    "\n",
    "def parse_marker_table(csv_path: Path) -> Tuple[List[Dict[str, Any]], Dict[str, int], Dict[int, Dict[str, Any]], List[Tuple[int, int]]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        logger.warning('Marker-CSV %s ist leer oder konnte nicht gelesen werden.', csv_path)\n",
    "        return [], {}, {}, []\n",
    "\n",
    "    headers = list(rows[0].keys())\n",
    "\n",
    "    def _find_col(tokens: List[str], default: Optional[str] = None) -> Optional[str]:\n",
    "        opts = [col for col in headers]\n",
    "        if default and default in opts:\n",
    "            return default\n",
    "        for col in opts:\n",
    "            low = col.lower()\n",
    "            if any(tok in low for tok in tokens):\n",
    "                return col\n",
    "        return default if default in headers else None\n",
    "\n",
    "    include_col = _find_col(['include'], None)\n",
    "\n",
    "    include_rows: List[Dict[str, Any]] = []\n",
    "    for row in rows:\n",
    "        flag = row.get(include_col, 'TRUE') if include_col else 'TRUE'\n",
    "        if str(flag).strip().upper() in {'TRUE', '1', 'YES', 'Y', 'JA'}:\n",
    "            include_rows.append(row)\n",
    "\n",
    "    if not include_rows:\n",
    "        logger.warning('Keine Include==TRUE-EintrÃ¤ge in Marker-CSV: %s', csv_path)\n",
    "        return [], {}, {}, []\n",
    "\n",
    "    no_col = _find_col(['no'], 'No')\n",
    "    marker_col = _find_col(['marker', 'antibody', 'target'], 'Marker-Name')\n",
    "    fluor_col = _find_col(['fluor', 'dye'], None)\n",
    "    spill_col = _find_col(['spillover'], 'Spillover_from')\n",
    "\n",
    "    mapping: Dict[str, int] = {}\n",
    "    for idx, row in enumerate(include_rows):\n",
    "        channel_no = str(row.get(no_col, '')).strip() if no_col else str(idx + 1)\n",
    "        if channel_no:\n",
    "            mapping[channel_no] = idx\n",
    "\n",
    "    channel_info: Dict[int, Dict[str, Any]] = {}\n",
    "    spill_pairs: List[Tuple[int, int]] = []\n",
    "\n",
    "    for idx, row in enumerate(include_rows):\n",
    "        channel_no = str(row.get(no_col, '')).strip() if no_col else str(idx + 1)\n",
    "        marker_name = str(row.get(marker_col, '')).strip() if marker_col else ''\n",
    "        fluor_name = str(row.get(fluor_col, '')).strip() if fluor_col else ''\n",
    "        spill_from = str(row.get(spill_col, '')).strip() if spill_col else ''\n",
    "\n",
    "        channel_info[idx] = dict(\n",
    "            no=channel_no or str(idx + 1),\n",
    "            marker=marker_name,\n",
    "            fluor=fluor_name,\n",
    "            spillover_from=spill_from,\n",
    "        )\n",
    "\n",
    "        donor_no = spill_from\n",
    "        if donor_no and donor_no.lower() != 'na':\n",
    "            donor_idx = mapping.get(donor_no)\n",
    "            if donor_idx is None:\n",
    "                logger.warning('Spillover-Referenz %s (fÃ¼r Ziel No=%s) ist nicht in Include==TRUE enthalten.', donor_no, channel_no)\n",
    "                continue\n",
    "            spill_pairs.append((idx, donor_idx))\n",
    "\n",
    "    return include_rows, mapping, channel_info, spill_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6317ff6",
   "metadata": {},
   "source": [
    "## 3. CSV-Analyse & Index-Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92602166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:09:54 | INFO     | Include==TRUE: 96 KanÃ¤le\n",
      "16:09:54 | INFO     | Spillover-Paare (gemappt): []\n",
      "16:09:54 | INFO     | Ãœberspringe KanÃ¤le (0-based): [0, 1, 2]\n",
      "16:09:54 | INFO     | Spillover-Paare (gemappt): []\n",
      "16:09:54 | INFO     | Ãœberspringe KanÃ¤le (0-based): [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "include_rows, id_mapping, channel_info, spill_pairs = parse_marker_table(MARKER_CSV)\n",
    "logger.info('Include==TRUE: %d KanÃ¤le', len(include_rows))\n",
    "logger.info('Spillover-Paare (gemappt): %s', spill_pairs)\n",
    "\n",
    "skip_zero_based = [max(0, sc - 1) for sc in SKIP_CHANNELS]\n",
    "logger.info('Ãœberspringe KanÃ¤le (0-based): %s', skip_zero_based)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95208fc",
   "metadata": {},
   "source": [
    "## 3.5 DIAGNOSE: Input-File prÃ¼fen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893d1863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-File: fused_decon.tif\n",
      "Input-GrÃ¶ÃŸe: 3.21 GB (3,448,948,021 Bytes)\n",
      "\n",
      "ðŸ” DETAILLIERTE TIFF-ANALYSE:\n",
      "  Anzahl Series: 96\n",
      "  Anzahl Pages: 96\n",
      "\n",
      "  Series[0]:\n",
      "    Axes: YX\n",
      "    Shape: (3947, 7607)\n",
      "    Dtype: uint16\n",
      "\n",
      "  âš ï¸ WARNUNG: 96 Pages gefunden!\n",
      "     MÃ¶glicherweise sind die Channels als separate Pages gespeichert.\n",
      "     Erste 3 Pages:\n",
      "       Page 0: Shape=(3947, 7607), Dtype=uint16\n",
      "       Page 1: Shape=(3947, 7607), Dtype=uint16\n",
      "       Page 2: Shape=(3947, 7607), Dtype=uint16\n",
      "\n",
      "  Erwartete GrÃ¶ÃŸe (1 Channel, unkomprimiert): 0.06 GB\n",
      "  Erwartete GrÃ¶ÃŸe (83 Channels, unkomprimiert): 4.64 GB\n",
      "\n",
      "============================================================\n",
      "ðŸš¨ DIAGNOSE:\n",
      "  âœ… File hat 96 Pages â†’ wahrscheinlich 83 Channels als separate Pages!\n",
      "  âŒ ABER: tifffile.series[0] liest nur ERSTE Page/Channel!\n",
      "  ðŸ”§ FIX: load_ome_to_chw() muss ALLE Pages laden, nicht nur series[0]!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# PrÃ¼fe Input-File GrÃ¶ÃŸe\n",
    "input_size_bytes = INPUT_PATH.stat().st_size\n",
    "input_size_gb = input_size_bytes / (1024**3)\n",
    "print(f\"Input-File: {INPUT_PATH.name}\")\n",
    "print(f\"Input-GrÃ¶ÃŸe: {input_size_gb:.2f} GB ({input_size_bytes:,} Bytes)\")\n",
    "\n",
    "# DETAILLIERTE TIFF-ANALYSE\n",
    "with TiffFile(str(INPUT_PATH)) as tf:\n",
    "    print(f\"\\nðŸ” DETAILLIERTE TIFF-ANALYSE:\")\n",
    "    print(f\"  Anzahl Series: {len(tf.series)}\")\n",
    "    print(f\"  Anzahl Pages: {len(tf.pages)}\")\n",
    "    \n",
    "    # Erste Series (Standard)\n",
    "    series = tf.series[0]\n",
    "    print(f\"\\n  Series[0]:\")\n",
    "    print(f\"    Axes: {series.axes}\")\n",
    "    print(f\"    Shape: {series.shape}\")\n",
    "    print(f\"    Dtype: {series.dtype}\")\n",
    "    \n",
    "    # PrÃ¼fe, ob Pages separate Channels sind\n",
    "    if len(tf.pages) > 1:\n",
    "        print(f\"\\n  âš ï¸ WARNUNG: {len(tf.pages)} Pages gefunden!\")\n",
    "        print(f\"     MÃ¶glicherweise sind die Channels als separate Pages gespeichert.\")\n",
    "        print(f\"     Erste 3 Pages:\")\n",
    "        for i, page in enumerate(tf.pages[:3]):\n",
    "            print(f\"       Page {i}: Shape={page.shape}, Dtype={page.dtype}\")\n",
    "    \n",
    "    # Berechne erwartete GrÃ¶ÃŸe WENN 83 Channels\n",
    "    expected_bytes_single = 1\n",
    "    for dim in series.shape:\n",
    "        expected_bytes_single *= dim\n",
    "    expected_bytes_single *= np.dtype(series.dtype).itemsize\n",
    "    \n",
    "    expected_bytes_83ch = expected_bytes_single * 83\n",
    "    expected_gb_83ch = expected_bytes_83ch / (1024**3)\n",
    "    \n",
    "    print(f\"\\n  Erwartete GrÃ¶ÃŸe (1 Channel, unkomprimiert): {expected_bytes_single / (1024**3):.2f} GB\")\n",
    "    print(f\"  Erwartete GrÃ¶ÃŸe (83 Channels, unkomprimiert): {expected_gb_83ch:.2f} GB\")\n",
    "    \n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸš¨ DIAGNOSE:\")\n",
    "if len(tf.pages) > 80:\n",
    "    print(f\"  âœ… File hat {len(tf.pages)} Pages â†’ wahrscheinlich 83 Channels als separate Pages!\")\n",
    "    print(f\"  âŒ ABER: tifffile.series[0] liest nur ERSTE Page/Channel!\")\n",
    "    print(f\"  ðŸ”§ FIX: load_ome_to_chw() muss ALLE Pages laden, nicht nur series[0]!\")\n",
    "else:\n",
    "    print(f\"  âŒ File hat NUR {len(tf.pages)} Pages â†’ Script 1 hat falsch gespeichert!\")\n",
    "    print(f\"  ðŸ”§ FIX: Script 1 muss Channels als Multi-Page TIFF oder mit C-Achse speichern!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b33a6",
   "metadata": {},
   "source": [
    "## 4. Spillover entfernen & speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f837e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:09:55 | INFO     | Lade OME-TIFF: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\multicycle_mosaics\\decon2D\\decon2D_fused\\fused_decon.tif\n",
      "16:09:55 | INFO     | Multi-Page TIFF erkannt: 96 Pages (Channels)\n",
      "16:09:55 | INFO     | Lade 96 Channels...\n",
      "16:09:55 | INFO     | Multi-Page TIFF erkannt: 96 Pages (Channels)\n",
      "16:09:55 | INFO     | Lade 96 Channels...\n",
      "16:09:55 | INFO     |   10/96 Channels geladen\n",
      "16:09:55 | INFO     |   10/96 Channels geladen\n",
      "16:09:56 | INFO     |   20/96 Channels geladen\n",
      "16:09:56 | INFO     |   20/96 Channels geladen\n",
      "16:09:57 | INFO     |   30/96 Channels geladen\n",
      "16:09:57 | INFO     |   30/96 Channels geladen\n",
      "16:09:58 | INFO     |   40/96 Channels geladen\n",
      "16:09:58 | INFO     |   40/96 Channels geladen\n",
      "16:09:58 | INFO     |   50/96 Channels geladen\n",
      "16:09:58 | INFO     |   50/96 Channels geladen\n",
      "16:09:59 | INFO     |   60/96 Channels geladen\n",
      "16:09:59 | INFO     |   60/96 Channels geladen\n",
      "16:10:00 | INFO     |   70/96 Channels geladen\n",
      "16:10:00 | INFO     |   70/96 Channels geladen\n",
      "16:10:01 | INFO     |   80/96 Channels geladen\n",
      "16:10:01 | INFO     |   80/96 Channels geladen\n",
      "16:10:02 | INFO     |   90/96 Channels geladen\n",
      "16:10:02 | INFO     |   90/96 Channels geladen\n",
      "16:10:03 | INFO     | âœ… Alle 96 Channels geladen: Shape=(96, 3947, 7607), Dtype=uint16\n",
      "16:10:03 | INFO     | âœ… Alle 96 Channels geladen: Shape=(96, 3947, 7607), Dtype=uint16\n",
      "16:10:23 | INFO     | Starte Spillover-Korrektur mit ROI-LS (V7-Mechanik).\n",
      "16:10:23 | INFO     | Starte Spillover-Korrektur mit ROI-LS (V7-Mechanik).\n",
      "Spillover: 0it [00:00, ?it/s]\n",
      "16:10:24 | INFO     | Spillover-Korrektur abgeschlossen. Konvertiere zurÃ¼ck in uint16.\n",
      "\n",
      "16:10:24 | INFO     | Spillover-Korrektur abgeschlossen. Konvertiere zurÃ¼ck in uint16.\n",
      "16:12:23 | INFO     | OME-TIFF gespeichert: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\fused_decon_spillover_corrected.ome.tif\n",
      "16:12:23 | INFO     | OME-TIFF gespeichert: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\fused_decon_spillover_corrected.ome.tif\n",
      "16:12:48 | INFO     | KanÃ¤le exportiert: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_channels\n",
      "16:12:48 | INFO     | Spillover-Log gespeichert: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_coefficients.json\n",
      "16:12:48 | INFO     | Workflow fertig.\n",
      "16:12:48 | INFO     | KanÃ¤le exportiert: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_channels\n",
      "16:12:48 | INFO     | Spillover-Log gespeichert: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_coefficients.json\n",
      "16:12:48 | INFO     | Workflow fertig.\n",
      "16:12:48 | INFO     |   OME  : C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\fused_decon_spillover_corrected.ome.tif\n",
      "16:12:48 | INFO     |   ChDir: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_channels\n",
      "16:12:48 | INFO     |   Spill: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_coefficients.json\n",
      "16:12:48 | INFO     |   OME  : C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\fused_decon_spillover_corrected.ome.tif\n",
      "16:12:48 | INFO     |   ChDir: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_channels\n",
      "16:12:48 | INFO     |   Spill: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_coefficients.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stack_raw, orig_dtype, channel_names = load_ome_to_chw(INPUT_PATH)\n",
    "if stack_raw.shape[0] != len(include_rows):\n",
    "    logger.warning('Stack-Kanalanzahl (%d) entspricht nicht Include==TRUE (%d). Bitte prÃ¼fen!', stack_raw.shape[0], len(include_rows))\n",
    "\n",
    "stack_work = stack_raw.copy()\n",
    "\n",
    "_unknown_tokens = {'nan', 'none', 'unknown', ''}\n",
    "\n",
    "def _clean_text(value: Any) -> str:\n",
    "    if value is None:\n",
    "        return ''\n",
    "    text = str(value).strip()\n",
    "    if not text or text.lower() in _unknown_tokens:\n",
    "        return ''\n",
    "    return text\n",
    "\n",
    "def _channel_label(idx: int) -> str:\n",
    "    info = channel_info.get(idx, {})\n",
    "    marker = _clean_text(info.get('marker'))\n",
    "    fluor = _clean_text(info.get('fluor'))\n",
    "    parts = [p for p in (marker, fluor) if p]\n",
    "    if parts:\n",
    "        return ' | '.join(parts)\n",
    "    if idx < len(channel_names):\n",
    "        return channel_names[idx]\n",
    "    return f'C{idx+1:02d}'\n",
    "\n",
    "def _safe_token(text: str) -> str:\n",
    "    cleaned = text.replace('|', '_').replace('/', '_').replace('\\\\', '_')\n",
    "    cleaned = cleaned.replace(' ', '_')\n",
    "    return ''.join(ch if ch.isalnum() or ch in {'_', '-'} else '_' for ch in cleaned)\n",
    "\n",
    "channel_labels = [_channel_label(i) for i in range(stack_work.shape[0])]\n",
    "\n",
    "logger.info('Starte Spillover-Korrektur mit ROI-LS (V7-Mechanik).')\n",
    "spill_results: List[Dict[str, Any]] = []\n",
    "for target_idx, donor_idx in tqdm(spill_pairs, desc='Spillover'):\n",
    "    if target_idx >= stack_work.shape[0] or donor_idx >= stack_work.shape[0]:\n",
    "        logger.warning('Paar auÃŸerhalb des Stacks: target=%d donor=%d', target_idx, donor_idx)\n",
    "        continue\n",
    "    if target_idx in skip_zero_based or donor_idx in skip_zero_based:\n",
    "        logger.info('Ãœberspringe Paar target=%d donor=%d (Skip-Liste)', target_idx, donor_idx)\n",
    "        continue\n",
    "\n",
    "    roi_before, _ = extract_roi_patch(stack_work[target_idx], **ROI_CFG)\n",
    "    coeff, stats = estimate_coeff_roi(\n",
    "        stack_work[target_idx],\n",
    "        stack_work[donor_idx],\n",
    "        roi_cfg=ROI_CFG,\n",
    "        pad=ROI_PAD,\n",
    "    )\n",
    "    apply_spillover(stack_work, donor_idx, target_idx, coeff)\n",
    "    roi_after, _ = extract_roi_patch(stack_work[target_idx], **ROI_CFG)\n",
    "\n",
    "    target_info = channel_info.get(target_idx, {})\n",
    "    donor_info = channel_info.get(donor_idx, {})\n",
    "    target_marker = _clean_text(target_info.get('marker')) or (channel_names[target_idx] if target_idx < len(channel_names) else f'C{target_idx+1:02d}')\n",
    "    donor_marker = _clean_text(donor_info.get('marker')) or (channel_names[donor_idx] if donor_idx < len(channel_names) else f'C{donor_idx+1:02d}')\n",
    "    target_fluor = _clean_text(target_info.get('fluor'))\n",
    "    donor_fluor = _clean_text(donor_info.get('fluor'))\n",
    "\n",
    "    stats.update(\n",
    "        target_idx=int(target_idx),\n",
    "        donor_idx=int(donor_idx),\n",
    "        roi_sum_after=float(np.sum(roi_after)),\n",
    "        roi_mean_before=float(np.mean(roi_before)),\n",
    "        roi_mean_after=float(np.mean(roi_after)),\n",
    "        target_marker=target_marker,\n",
    "        target_fluor=target_fluor,\n",
    "        donor_marker=donor_marker,\n",
    "        donor_fluor=donor_fluor,\n",
    "    )\n",
    "    spill_results.append(stats)\n",
    "\n",
    "    ratio_cap_disp = stats.get('coeff_cap_ratio')\n",
    "    if ratio_cap_disp is None or ratio_cap_disp < 0:\n",
    "        ratio_cap_disp = float('inf')\n",
    "    target_desc = target_marker if not target_fluor else f\"{target_marker} [{target_fluor}]\"\n",
    "    donor_desc = donor_marker if not donor_fluor else f\"{donor_marker} [{donor_fluor}]\"\n",
    "    logger.info(\n",
    "        'C%d (%s) <- C%d (%s) | coeff=%.4f (raw %.4f, cap %.4f) | ROI-mean %.3fâ†’%.3f',\n",
    "        target_idx + 1,\n",
    "        target_desc,\n",
    "        donor_idx + 1,\n",
    "        donor_desc,\n",
    "        stats['coeff'],\n",
    "        stats['coeff_raw'],\n",
    "        ratio_cap_disp,\n",
    "        stats['roi_mean_before'],\n",
    "        stats['roi_mean_after'],\n",
    "    )\n",
    "\n",
    "logger.info('Spillover-Korrektur abgeschlossen. Konvertiere zurÃ¼ck in %s.', orig_dtype)\n",
    "\n",
    "data_out = convert_to_dtype(stack_work, orig_dtype)\n",
    "spillover_dir, ome_path, channel_dir, spill_path = build_output_base(INPUT_PATH)\n",
    "channel_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metadata_labels = channel_labels if channel_labels else channel_names\n",
    "metadata = {'axes': 'CYX', 'Channel': {'Name': metadata_labels}}\n",
    "tiff.imwrite(str(ome_path), data_out, photometric='minisblack', metadata=metadata)\n",
    "logger.info('OME-TIFF gespeichert: %s', ome_path)\n",
    "\n",
    "for ci in range(data_out.shape[0]):\n",
    "    info = channel_info.get(ci, {})\n",
    "    marker_name = _clean_text(info.get('marker')) or (channel_names[ci] if ci < len(channel_names) else f'C{ci+1:02d}')\n",
    "    fluor_name = _clean_text(info.get('fluor'))\n",
    "    parts = [p for p in (marker_name, fluor_name) if p]\n",
    "    if not parts:\n",
    "        parts = [channel_labels[ci]]\n",
    "    label_core = '_'.join(_safe_token(part) for part in parts)\n",
    "    label = f\"C{ci+1:03d}_{label_core}\" if label_core else f\"C{ci+1:03d}\"\n",
    "    tiff.imwrite(str(channel_dir / f\"{label}.tif\"), data_out[ci], photometric='minisblack')\n",
    "logger.info('KanÃ¤le exportiert: %s', channel_dir)\n",
    "\n",
    "with open(spill_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(\n",
    "        dict(\n",
    "            pairs=spill_results,\n",
    "            params=dict(\n",
    "                roi=ROI_CFG,\n",
    "                pad=ROI_PAD,\n",
    "                percentile_q=PERCENTILE_Q,\n",
    "                safety_factor=SAFETY_FACTOR,\n",
    "                max_coeff=MAX_COEFF,\n",
    "            ),\n",
    "        ),\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "logger.info('Spillover-Log gespeichert: %s', spill_path)\n",
    "\n",
    "logger.info('Workflow fertig.')\n",
    "logger.info('  OME  : %s', ome_path)\n",
    "logger.info('  ChDir: %s', channel_dir)\n",
    "logger.info('  Spill: %s', spill_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055af377",
   "metadata": {},
   "source": [
    "## 5. Ergebnis-Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e649e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OME-TIFF: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\fused_decon_spillover_corrected.ome.tif\n",
      "Channel-Verzeichnis: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_channels\n",
      "Spillover-Log: C:\\Users\\researcher\\data\\Epoxy_CyNif\\Epoxy_CyNif\\data\\export\\sample_197\\spillover\\spillover_coefficients.json\n"
     ]
    }
   ],
   "source": [
    "print('OME-TIFF:', ome_path)\n",
    "print('Channel-Verzeichnis:', channel_dir)\n",
    "print('Spillover-Log:', spill_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Epoxy_CyNif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

