{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da41b5f3",
   "metadata": {},
   "source": [
    "# CyLinter Pipeline - GUI-basierter Workflow\n",
    "\n",
    "**Dieses Notebook steuert CyLinter QC ausschlie?lich ?ber GUIs. Konfiguration (Zelle 8) ? Ausf?hrung (Zelle 9); Codezellen bleiben unver?ndert.**\n",
    "\n",
    "---\n",
    "\n",
    "## ?? Notebook-?berblick\n",
    "| # | Typ | Zweck | Pflicht? |\n",
    "|---|-----|-------|----------|\n",
    "|1|MD|?bersicht & Workflow-Hinweise|??|\n",
    "|2|Code|Setup & Pfade (BASE_DIR, CONFIG_PATH, markers.csv)|? Run All|\n",
    "|3|Code|`cylinter_config.yml` + `markers.csv` laden/validieren|? Run All|\n",
    "|4|MD|Kurze Erkl?rung zur Config|??|\n",
    "|5|Code|Manuelle Quantifizierung & CSV-Erstellung|?? Optional|\n",
    "|6|Code|Pre-Flight Check (Pfad- & Schema-Validierung)|? Run All|\n",
    "|7|MD|Intro zur Konfigurations-GUI|??|\n",
    "|8|Code|**?? Konfig-GUI** (Marker-Subset w?hlen & speichern)|?? Tag 1|\n",
    "|9|Code|**?? PIPELINE-START** (Checkpoint-aware)|?? Tag 1|\n",
    "|10|Code|**STOP** ? Tag?1 endet hier|??|\n",
    "|11|MD|Intro zur erweiterten Steuerung|??|\n",
    "|12|Code|**?? Erweiterte GUI** (Tag?2+ / Fehlerkorrektur)|?? Tag 2+|\n",
    "|13|MD|Hinweis zum Parquet-Viewer|?? Optional|\n",
    "|14|Code|Parquet Viewer (Checkpoint-Dateien inspizieren)|?? Optional|\n",
    "\n",
    "---\n",
    "\n",
    "## ?? WORKFLOW TAG 1 (Erste Analyse)\n",
    "\n",
    "### 1?? Notebook starten\n",
    "```\n",
    "Kernel ? Restart & Run All\n",
    "```\n",
    "- ? F?hrt Zellen 2?6 aus (Setup, Config-Load, optionale manuelle Quantifizierung, Pre-Flight-Check).\n",
    "- ? L?dt Konfigurations-GUI (Zelle 8) und Pipeline-Start (Zelle 9).\n",
    "- ? **Stoppt an der STOP-Zelle** (verhindert versehentlichen Tag-2-Start).\n",
    "\n",
    "### 2?? Marker ausw?hlen (Zelle 8)\n",
    "**Quick-Optionen:** Alle Marker, Alle au?er AF, Nur Bio (Counts basieren auf `markers.csv`).  \n",
    "**Custom-Option:** Bio-Marker per Checkbox (AF wird automatisch ausgeschlossen).  \n",
    "**Button ??? Speichern & Bereit?:** Schreibt `markersToExclude` in `cylinter_config.yml`, startet keine Pipeline.\n",
    "\n",
    "### 3?? Pipeline starten (Zelle 9)\n",
    "- Zeigt Marker-Zusammenfassung und 15 Module.\n",
    "- **Button ??? Pipeline starten?** startet beim ersten fehlenden Checkpoint und l?uft bis `curateThumbnails`.\n",
    "- Interactive Module (`selectROIs`, `setContrast`, `gating`) ?ffnen GUIs.\n",
    "\n",
    "**?? Tag?1 Tipp:** Pipeline nicht abbrechen; Checkpoints werden nur beim vollst?ndigen Lauf geschrieben.\n",
    "\n",
    "---\n",
    "\n",
    "## ?? WORKFLOW TAG 2+ (Neue Marker oder Fehlerkorrektur)\n",
    "\n",
    "1?? Notebook neu starten, nur Zellen 2?3 ausf?hren (Setup + Config).  \n",
    "2?? Zelle 12 ?ffnen ? Erweiterte GUI.  \n",
    "3?? Tab 1: Marker erg?nzen/mergen (Backup wird erstellt).  \n",
    "4?? Tab 2: Checkpoints gezielt l?schen, Startmodul w?hlen, ?Run Selected?.  \n",
    "\n",
    "Gating nutzt vorhandene Thresholds aus `cylinter_report.yml`; nur Marker ohne Threshold werden neu gegated.\n",
    "\n",
    "---\n",
    "\n",
    "## ?? WORKFLOW FEHLERKORREKTUR (einzelnes Modul neu starten)\n",
    "\n",
    "- Notebook neu starten ? Zellen 2?3 ausf?hren.  \n",
    "- Zelle 12 (Tab 2): fehlerhaftes Modul resetten, Startmodul w?hlen ? ?Run Selected?.  \n",
    "- Pipeline l?uft bis Ende, ?berspringt bestehende Checkpoints.\n",
    "\n",
    "---\n",
    "\n",
    "## ?? Eingaben & Modul-Liste\n",
    "- Erwartet `cylinter_config.yml` und `markers.csv` im Notebook-Ordner; Checkpoints/Reports: `cylinter_output_prune_test/`.\n",
    "- Pipeline-Module (15):\n",
    "```\n",
    "1. aggregateData\n",
    "2. selectROIs (GUI)\n",
    "3. intensityFilter\n",
    "4. areaFilter\n",
    "5. cycleCorrelation\n",
    "6. logTransform\n",
    "7. pruneOutliers\n",
    "8. metaQC\n",
    "9. PCA\n",
    "10. setContrast (GUI)\n",
    "11. gating (GUI)\n",
    "12. clustering\n",
    "13. clustermap\n",
    "14. frequencyStats\n",
    "15. curateThumbnails\n",
    "```\n",
    "\n",
    "**Interactive Module (?ffnen GUIs):**\n",
    "- `selectROIs`, `setContrast`, `gating`\n",
    "\n",
    "---\n",
    "\n",
    "##  CHECKPOINT-SYSTEM VERSTEHEN\n",
    "\n",
    "**Was sind Checkpoints?**\n",
    "- Nach jedem erfolgreichen Modul speichert CyLinter eine `.parquet` Datei\n",
    "- Speicherort: `cylinter_output_prune_test/checkpoints/`\n",
    "- Format: `module_name.parquet` (z.B. `aggregateData.parquet`, `gating.parquet`)\n",
    "\n",
    "**Wie funktioniert es?**\n",
    "- Beim Pipeline-Start pr?ft CyLinter, welche Checkpoints existieren\n",
    "- Module mit Checkpoints werden **automatisch ?bersprungen**\n",
    "- Nur Module ohne Checkpoint oder mit gel?schtem Checkpoint laufen neu\n",
    "\n",
    "**Warum ist das n?tzlich?**\n",
    "- ? **Zeit sparen:** Keine unn?tigen Neuberechnungen\n",
    "- ? **Incremental Work:** Neue Marker hinzuf?gen ohne alles neu zu machen\n",
    "- ? **Fehlerkorrektur:** Einzelne Module isoliert neu starten\n",
    "- ? **Experimente:** Verschiedene Parameter testen (z.B. Clustering-Params)\n",
    "\n",
    "**Gating-Thresholds:**\n",
    "- Werden in `cylinter_report.yml` gespeichert (nicht in Checkpoint!)\n",
    "- Persistieren ?ber Pipeline-L?ufe hinweg\n",
    "- Erm?glichen TAG 2 Workflow: Neue Marker ohne alte Marker neu zu gaten\n",
    "\n",
    "---\n",
    "\n",
    "## ?? WICHTIGE HINWEISE\n",
    "\n",
    "### CyLinter's Pipeline-Verhalten:\n",
    "- **`--module X` setzt nur STARTPUNKT, nicht Endpunkt!**\n",
    "- Pipeline l?uft IMMER vom Startmodul bis zum Ende (15. Modul)\n",
    "- Sie k?nnen KEINE Modul-Teilmenge ausw?hlen (z.B. nur Module 3-5)\n",
    "- Checkpoints erm?glichen das ?berspringen: Pipeline l?uft alle 15 durch, aber existierende Checkpoints werden ?bersprungen\n",
    "\n",
    "### TAG 1 vs TAG 2:\n",
    "- **TAG 1:** Vollst?ndige Erstanalyse\n",
    "  - Zelle 8: Marker w?hlen\n",
    "  - Zelle 9: Pipeline starten (aggregateData ? Ende)\n",
    "  - Alle interactive Module durchlaufen\n",
    "  \n",
    "- **TAG 2+:** Inkrementelle Updates\n",
    "  - Zelle 12 Tab 1: Neue Marker mergen\n",
    "  - Zelle 12 Tab 2: Checkpoint l?schen + Pipeline starten\n",
    "  - Checkpoints erm?glichen ?berspringen bereits berechneter Schritte\n",
    "\n",
    "### Laufzeiten (ca.):\n",
    "- **TAG 1 (AF ausgeschlossen):** ~40-50 Min\n",
    "- **TAG 1 (alle Marker):** ~90-120 Min\n",
    "- **TAG 2 (neue Marker):** ~10-15 Min\n",
    "- **Fehlerkorrektur (1 Modul):** ~2-10 Min (je nach Modul)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970dba5b-94f0-4cc2-830a-c9200afafec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Zelle 1: Modulimporte, Pfaddefinitionen und automatische Dateierkennung ---\n",
      "Projekt-Basisverzeichnis (BASE_DIR): /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191\n",
      "Konfigurationsdatei (CONFIG_PATH): /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/cylinter_config.yml\n",
      "Marker CSV (MARKERS_CSV_PATH): /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/markers.csv\n",
      "\n",
      "--- ÃœberprÃ¼fe vorhandene Dateien ---\n",
      "CSV: 0 Datei(en) gefunden\n",
      "TIF: 2 Datei(en) gefunden\n",
      "  - fused_decon_AF_cleaned.ome.tif\n",
      "  - .DS_Store\n",
      "SEG: 2 Datei(en) gefunden\n",
      "  - .DS_Store\n",
      "  - fused_decon_AF_cleaned.tiff\n",
      "MASK: 2 Datei(en) gefunden\n",
      "  - .DS_Store\n",
      "  - fused_decon_AF_cleaned.tiff\n",
      "\n",
      "--- PrÃ¼fe auf .tiff Dateien (falls vorhanden) ---\n",
      "TIF: Keine .tiff Dateien gefunden (OK)\n",
      "SEG: Konvertiere 1 .tiff Datei(en)...\n",
      "   â†’ Konvertiert: fused_decon_AF_cleaned.tiff â†’ fused_decon_AF_cleaned.tif\n",
      "MASK: Konvertiere 1 .tiff Datei(en)...\n",
      "   â†’ Konvertiert: fused_decon_AF_cleaned.tiff â†’ fused_decon_AF_cleaned.tif\n",
      "\n",
      "--- Dateivorbereitung abgeschlossen ---\n",
      "\n",
      "--- PrÃ¼fe markers.csv Format ---\n",
      "  Original markers.csv gesichert als: markers_original_backup.csv\n",
      "  Geladene markers.csv: 76 EintrÃ¤ge\n",
      "  âœ… markers.csv hat _cX Suffixe (korrekt fÃ¼r aggregateData): ['DAPI_c3', 'AF1_c3', 'AF2_c3']\n",
      "  â†’ CyLinter aggregateData kann CSV-Spalten finden!\n",
      "\n",
      "--- PrÃ¼fung abgeschlossen ---\n",
      "Zelle 1 erfolgreich ausgefÃ¼hrt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zelle 1: Imports und Basiskonfiguration (DYNAMISCH fÃ¼r beliebige Samples)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import time\n",
    "import os\n",
    "import subprocess # FÃ¼r CLI-Aufrufe\n",
    "import shutil\n",
    "\n",
    "print(\"--- Zelle 1: Modulimporte, Pfaddefinitionen und automatische Dateierkennung ---\")\n",
    "BASE_DIR = Path.cwd()\n",
    "CONFIG_PATH = BASE_DIR / \"cylinter_config.yml\"\n",
    "MARKERS_CSV_PATH = BASE_DIR / \"markers.csv\"\n",
    "\n",
    "print(f\"Projekt-Basisverzeichnis (BASE_DIR): {BASE_DIR}\")\n",
    "print(f\"Konfigurationsdatei (CONFIG_PATH): {CONFIG_PATH}\")\n",
    "print(f\"Marker CSV (MARKERS_CSV_PATH): {MARKERS_CSV_PATH}\")\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"cylinter_config.yml nicht gefunden: {CONFIG_PATH}\")\n",
    "if not MARKERS_CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"markers.csv nicht gefunden: {MARKERS_CSV_PATH}\")\n",
    "\n",
    "# Sicherstellen, dass die Eingabeordner existieren\n",
    "(BASE_DIR / 'csv').mkdir(exist_ok=True)\n",
    "(BASE_DIR / 'tif').mkdir(exist_ok=True)\n",
    "(BASE_DIR / 'seg').mkdir(exist_ok=True)\n",
    "(BASE_DIR / 'mask').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n--- ÃœberprÃ¼fe vorhandene Dateien ---\")\n",
    "# ZÃ¤hle Dateien in jedem Ordner\n",
    "for folder in ['csv', 'tif', 'seg', 'mask']:\n",
    "    folder_path = BASE_DIR / folder\n",
    "    if folder_path.exists():\n",
    "        files = list(folder_path.glob('*'))\n",
    "        files = [f for f in files if f.is_file()]  # Nur Dateien, keine Ordner\n",
    "        print(f\"{folder.upper()}: {len(files)} Datei(en) gefunden\")\n",
    "        if files:\n",
    "            for f in files[:3]:  # Zeige max. 3 Dateien\n",
    "                print(f\"  - {f.name}\")\n",
    "            if len(files) > 3:\n",
    "                print(f\"  ... und {len(files)-3} weitere\")\n",
    "\n",
    "# --- OPTIONALE DATEIVORBEREITUNG ---\n",
    "# Falls .tiff Dateien vorhanden sind, diese zu .tif konvertieren\n",
    "print(\"\\n--- PrÃ¼fe auf .tiff Dateien (falls vorhanden) ---\")\n",
    "tiff_converted = False\n",
    "for folder in ['tif', 'seg', 'mask']:\n",
    "    folder_path = BASE_DIR / folder\n",
    "    if folder_path.exists():\n",
    "        tiff_files = list(folder_path.glob(\"*.tiff\"))\n",
    "        if tiff_files:\n",
    "            print(f\"{folder.upper()}: Konvertiere {len(tiff_files)} .tiff Datei(en)...\")\n",
    "            for tiff_file in tiff_files:\n",
    "                tif_file = tiff_file.with_suffix('.tif')\n",
    "                shutil.move(tiff_file, tif_file)\n",
    "                print(f\"   â†’ Konvertiert: {tiff_file.name} â†’ {tif_file.name}\")\n",
    "                tiff_converted = True\n",
    "        else:\n",
    "            print(f\"{folder.upper()}: Keine .tiff Dateien gefunden (OK)\")\n",
    "\n",
    "if not tiff_converted:\n",
    "    print(\"\\nKeine .tiff Konvertierung nÃ¶tig - alle Dateien bereits im .tif Format.\")\n",
    "\n",
    "print(\"\\n--- Dateivorbereitung abgeschlossen ---\")\n",
    "\n",
    "# --- WICHTIG: markers.csv NICHT modifizieren (muss OME-TIFF Namen entsprechen) ---\n",
    "print(\"\\n--- PrÃ¼fe markers.csv Format ---\")\n",
    "# KRITISCH: markers.csv muss die ORIGINALEN Kanalnamen aus OME-TIFF enthalten (OHNE _cX)!\n",
    "# Die _cX Suffixe gehÃ¶ren NUR in die CSV-Datei (Quantifizierung), NICHT in markers.csv\n",
    "\n",
    "import pandas as pd\n",
    "markers_original_path = BASE_DIR / \"markers.csv\"\n",
    "markers_backup_path = BASE_DIR / \"markers_original_backup.csv\"\n",
    "\n",
    "# Backup erstellen (nur einmal)\n",
    "if not markers_backup_path.exists():\n",
    "    import shutil\n",
    "    shutil.copy(markers_original_path, markers_backup_path)\n",
    "    print(f\"  Original markers.csv gesichert als: {markers_backup_path.name}\")\n",
    "\n",
    "# Markers.csv laden und prÃ¼fen\n",
    "df_markers = pd.read_csv(markers_original_path)\n",
    "print(f\"  Geladene markers.csv: {len(df_markers)} EintrÃ¤ge\")\n",
    "\n",
    "# CHECK & AUTO-FIX: PrÃ¼fe ob _cX vorhanden ist und fÃ¼ge hinzu falls nÃ¶tig\n",
    "sample_marker_names = df_markers['marker_name'].head(3).tolist()\n",
    "has_cycle_suffix = any('_c' in str(m) for m in sample_marker_names)\n",
    "\n",
    "if not has_cycle_suffix:\n",
    "    print(f\"  âš ï¸  markers.csv hat KEINE _cX Suffixe: {sample_marker_names}\")\n",
    "    print(f\"  â†’ FÃ¼ge _cX automatisch hinzu (fÃ¼r aggregateData)...\")\n",
    "    \n",
    "    # FÃ¼ge _cX Suffixe hinzu (z.B. \"DAPI\" â†’ \"DAPI_c1\")\n",
    "    df_markers['marker_name'] = (\n",
    "        df_markers['marker_name'].str.strip().str.replace(' ', '_', regex=False) + \n",
    "        '_c' + df_markers['cycle_number'].astype(str)\n",
    "    )\n",
    "    \n",
    "    # Speichern\n",
    "    df_markers.to_csv(markers_original_path, index=False)\n",
    "    print(f\"  âœ… markers.csv korrigiert: _cX Suffixe hinzugefÃ¼gt\")\n",
    "    print(f\"     Beispiele: {df_markers['marker_name'].head(3).tolist()}\")\n",
    "else:\n",
    "    print(f\"  âœ… markers.csv hat _cX Suffixe (korrekt fÃ¼r aggregateData): {sample_marker_names}\")\n",
    "    print(f\"  â†’ CyLinter aggregateData kann CSV-Spalten finden!\")\n",
    "\n",
    "# Leerzeichen entfernen (falls vorhanden)\n",
    "if df_markers['marker_name'].str.contains(' ', regex=False).any():\n",
    "    print(f\"  âš™ï¸  Entferne Leerzeichen aus Marker-Namen...\")\n",
    "    df_markers['marker_name'] = df_markers['marker_name'].str.strip().str.replace(' ', '_', regex=False)\n",
    "    df_markers.to_csv(markers_original_path, index=False)\n",
    "    print(f\"  âœ… Leerzeichen entfernt\")\n",
    "\n",
    "print(\"\\n--- PrÃ¼fung abgeschlossen ---\")\n",
    "print(\"Zelle 1 erfolgreich ausgefÃ¼hrt.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306f79ff-b7b2-4ef5-b754-7fedb18c3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Zelle 2: Lade und validiere Konfigurationsdaten aus YAML ---\n",
      "cylinter_config.yml erfolgreich als Dictionary geladen.\n",
      "Marker-Informationen aus /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/markers.csv geladen. 76 KanÃ¤le gefunden.\n",
      "  Beispiele (bereits mit _cX aus markers.csv): ['DAPI_c3', 'AF1_c3', 'AF2_c3', 'Vimentin_c3', 'CD3_c3']\n",
      "FÃ¼r Quantifizierung wird Sample-Dateiname verwendet: fused_decon_AF_cleaned\n",
      "  Erwarteter Pfad fÃ¼r Bild: /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/tif/fused_decon_AF_cleaned.ome.tif\n",
      "  Erwarteter Pfad fÃ¼r Zellmaske: /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/mask/fused_decon_AF_cleaned.tif\n",
      "  Erwarteter Pfad fÃ¼r Ausgabe-CSV: /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/csv/fused_decon_AF_cleaned.csv\n",
      "Zelle 2 erfolgreich ausgefÃ¼hrt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zelle 2: Konfigurationsobjekt erstellen (vereinfacht fÃ¼r Klarheit, da CLI die Config liest)\n",
    "print(\"--- Zelle 2: Lade und validiere Konfigurationsdaten aus YAML ---\")\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"FEHLER: {CONFIG_PATH} nicht gefunden. Zelle 1 erneut ausfÃ¼hren oder Pfad prÃ¼fen.\")\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f_yaml:\n",
    "        config_dict_from_yaml = yaml.safe_load(f_yaml) # Diese Variable wird in Zelle 2.5 verwendet\n",
    "    if config_dict_from_yaml is None:\n",
    "        raise ValueError(\"cylinter_config.yml konnte nicht geladen werden oder ist leer.\")\n",
    "    print(f\"cylinter_config.yml erfolgreich als Dictionary geladen.\")\n",
    "except Exception as e:\n",
    "    print(f\"FEHLER beim Laden der cylinter_config.yml: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    markers_df_for_quant = pd.read_csv(MARKERS_CSV_PATH)\n",
    "    if 'marker_name' not in markers_df_for_quant.columns:\n",
    "        raise ValueError(\"Spalte 'marker_name' nicht in markers.csv gefunden.\")\n",
    "    if 'cycle_number' not in markers_df_for_quant.columns:\n",
    "        raise ValueError(\"Spalte 'cycle_number' nicht in markers.csv gefunden.\")\n",
    "    \n",
    "    # WICHTIG: markers.csv enthÃ¤lt bereits _cX Suffixe (nach Auto-Fix in Zelle 1)\n",
    "    # Verwende diese DIREKT ohne weitere Modifikation\n",
    "    all_channel_names_from_markers_csv = (\n",
    "        markers_df_for_quant['marker_name'].str.strip().str.replace(' ', '_', regex=False)\n",
    "    ).tolist()\n",
    "    \n",
    "    print(f\"Marker-Informationen aus {MARKERS_CSV_PATH} geladen. {len(all_channel_names_from_markers_csv)} KanÃ¤le gefunden.\")\n",
    "    print(f\"  Beispiele (bereits mit _cX aus markers.csv): {all_channel_names_from_markers_csv[:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"FEHLER beim Laden der markers.csv: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    inDir_name_from_yaml = config_dict_from_yaml.get('inDir', '.')\n",
    "    inDir_for_quant = (BASE_DIR / Path(inDir_name_from_yaml)).resolve()\n",
    "    image_dir_name_from_yaml = config_dict_from_yaml.get('image_dir', 'tif')\n",
    "    mask_dir_name_from_yaml = config_dict_from_yaml.get('mask_dir', 'mask')\n",
    "    csv_dir_name_from_yaml = config_dict_from_yaml.get('csv_dir', 'csv')\n",
    "    smd = config_dict_from_yaml.get('sampleMetadata', {})\n",
    "    if not smd: raise ValueError(\"'sampleMetadata' fehlt in Config.\")\n",
    "    sample_key_from_yaml = list(smd.keys())[0]\n",
    "    sample_values_from_yaml = smd[sample_key_from_yaml]\n",
    "    SAMPLE_NAME_FOR_FILES_QUANT = sample_values_from_yaml[0]\n",
    "    print(f\"FÃ¼r Quantifizierung wird Sample-Dateiname verwendet: {SAMPLE_NAME_FOR_FILES_QUANT}\")\n",
    "    path_image_for_quant = inDir_for_quant / image_dir_name_from_yaml / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.ome.tif\"\n",
    "    path_cell_mask_for_quant = inDir_for_quant / mask_dir_name_from_yaml / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.tif\"\n",
    "    path_csv_output_for_quant = inDir_for_quant / csv_dir_name_from_yaml / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.csv\"\n",
    "    print(f\"  Erwarteter Pfad fÃ¼r Bild: {path_image_for_quant}\")\n",
    "    print(f\"  Erwarteter Pfad fÃ¼r Zellmaske: {path_cell_mask_for_quant}\")\n",
    "    print(f\"  Erwarteter Pfad fÃ¼r Ausgabe-CSV: {path_csv_output_for_quant}\")\n",
    "except KeyError as ke: print(f\"FEHLER: SchlÃ¼ssel in config nicht gefunden: {ke}\"); raise\n",
    "except Exception as e: print(f\"FEHLER beim Extrahieren von Werten aus Config: {e}\"); raise\n",
    "print(\"Zelle 2 erfolgreich ausgefÃ¼hrt.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491730aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Zelle 2: Konfiguration laden\n",
    "\n",
    "LÃ¤dt `cylinter_config.yml` und `markers.csv`, validiert Struktur und bereitet Pfade vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6659056c-3922-4b15-a93d-b86f6a2de2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Zelle 2.5: Manuelle Quantifizierung und Erstellung der CSV-Datei ---\n",
      "\n",
      "--- Suche vorhandene Dateien fÃ¼r Quantifizierung ---\n",
      "Verwende TIF: fused_decon_AF_cleaned.ome.tif\n",
      "Verwende MASK: fused_decon_AF_cleaned.tif\n",
      "Ausgabe CSV: fused_decon_AF_cleaned.csv\n",
      "--- Dateien gefunden ---\n",
      "\n",
      "FÃ¼r die Quantifizierung werden 48 KanÃ¤le verwendet (nach Ausschluss).\n",
      "  Ausgeschlossene Marker fÃ¼r Quant (aus config): ['AF1_c3', 'AF2_c3', 'AF1_c5', 'AF2_c5', 'AF1_c6']... (bis zu 5 gezeigt)\n",
      "  Zu quantifizierende Marker: ['DAPI_c3', 'Vimentin_c3', 'CD3_c3', 'MSH2_c3', 'DAPI_c5']... (bis zu 5 gezeigt)\n",
      "Lade Bild fÃ¼r Quantifizierung: /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/tif/fused_decon_AF_cleaned.ome.tif\n",
      "  Bildform roh: (76, 7624, 5788)\n",
      "  Bildform fÃ¼r Quantifizierung (H,W,C): (7624, 5788, 76)\n",
      "Lade Zellmaske fÃ¼r Quantifizierung: /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/mask/fused_decon_AF_cleaned.tif\n",
      "  Zellmaskenform: (7624, 5788), Max Label: 17431\n",
      "Berechne morphologische Eigenschaften...\n",
      "  Spalten direkt nach regionprops_table: ['label', 'area', 'centroid-0', 'centroid-1', 'major_axis_length', 'minor_axis_length', 'eccentricity', 'solidity', 'orientation']\n",
      "  Spalten nach Umbenennung: ['CellID', 'Area', 'Y_centroid', 'X_centroid', 'MajorAxisLength', 'MinorAxisLength', 'Eccentricity', 'Solidity', 'Orientation']\n",
      "\n",
      "Extrahiere IntensitÃ¤ten fÃ¼r 48 ausgewÃ¤hlte Marker pro Zelle...\n",
      "  Strategie: Mean (Standard) + 95th Percentile (fÃ¼r Membranmarker)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bfbe7c75714554b9bb3bcb3ee0fea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantifiziere Marker:   0%|          | 0/48 [00:00<?, ?marker/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ 29 Membranmarker bekamen zusÃ¤tzlich _p95 Spalten\n",
      "\n",
      "--- Stabilisiere IntensitÃ¤tswerte (ersetze â‰¤0 mit Minimum) ---\n",
      "  âœ“ DAPI_c3: 1 Werte ersetzt durch 12.5\n",
      "  âœ“ Vimentin_c3: 177 Werte ersetzt durch 0.917\n",
      "  âœ“ CD3_c3: 25 Werte ersetzt durch 0.555\n",
      "  âœ“ CD3_c3_p95: 46 Werte ersetzt durch 40.3\n",
      "  âœ“ MSH2_c3: 114 Werte ersetzt durch 0.138\n",
      "  âœ“ MSH2_c3_p95: 189 Werte ersetzt durch 90.6\n",
      "  âœ“ DAPI_c5: 1 Werte ersetzt durch 137\n",
      "  âœ“ CD45RA_c5: 1 Werte ersetzt durch 36.4\n",
      "  âœ“ CD45RA_c5_p95: 1 Werte ersetzt durch 320\n",
      "  âœ“ MLH1_c5: 12 Werte ersetzt durch 4.76\n",
      "  âœ“ MLH1_c5_p95: 14 Werte ersetzt durch 111\n",
      "  âœ“ DAPI_c6: 12 Werte ersetzt durch 366\n",
      "  âœ“ Epcam_c6: 100 Werte ersetzt durch 1.59\n",
      "  âœ“ Epcam_c6_p95: 149 Werte ersetzt durch 1.28e+03\n",
      "  âœ“ CD8a_c6: 100 Werte ersetzt durch 0.699\n",
      "  âœ“ CD8a_c6_p95: 149 Werte ersetzt durch 636\n",
      "  âœ“ Ki67_c6: 100 Werte ersetzt durch 1.17\n",
      "  âœ“ DAPI_c7: 12 Werte ersetzt durch 1.03e+03\n",
      "  âœ“ CD4_c7_p95: 3 Werte ersetzt durch 100\n",
      "  âœ“ IL17A_c7_p95: 3 Werte ersetzt durch 321\n",
      "  âœ“ CD163_c7_p95: 3 Werte ersetzt durch 323\n",
      "  âœ“ CD68_c8: 83 Werte ersetzt durch 2.35\n",
      "  âœ“ CD68_c8_p95: 123 Werte ersetzt durch 785\n",
      "  âœ“ CD56_c8: 20 Werte ersetzt durch 0.579\n",
      "  âœ“ CD56_c8_p95: 43 Werte ersetzt durch 42.8\n",
      "  âœ“ Laminin_c9: 712 Werte ersetzt durch 1.02\n",
      "  âœ“ CD69_c9: 58 Werte ersetzt durch 1.03\n",
      "  âœ“ CD69_c9_p95: 89 Werte ersetzt durch 123\n",
      "  âœ“ DAPI_c10: 7 Werte ersetzt durch 79.2\n",
      "  âœ“ CD66b_c10: 13 Werte ersetzt durch 11.5\n",
      "  âœ“ CD66b_c10_p95: 16 Werte ersetzt durch 1.23e+03\n",
      "  âœ“ CD103_c10: 13 Werte ersetzt durch 1.36\n",
      "  âœ“ CD103_c10_p95: 16 Werte ersetzt durch 167\n",
      "  âœ“ cMAF_c10: 13 Werte ersetzt durch 2.39\n",
      "  âœ“ DAPI_c11: 1 Werte ersetzt durch 11.4\n",
      "  âœ“ CD94_c11: 125 Werte ersetzt durch 0.831\n",
      "  âœ“ CD94_c11_p95: 171 Werte ersetzt durch 398\n",
      "  âœ“ FOXP3_c11: 79 Werte ersetzt durch 0.559\n",
      "  âœ“ FOXP3_c11_p95: 118 Werte ersetzt durch 78.4\n",
      "  âœ“ DAPI_c12: 1 Werte ersetzt durch 33.8\n",
      "  âœ“ CD20_c12: 113 Werte ersetzt durch 0.123\n",
      "  âœ“ CD20_c12_p95: 172 Werte ersetzt durch 51.9\n",
      "  âœ“ HLADR_c12: 109 Werte ersetzt durch 0.218\n",
      "  âœ“ HLADR_c12_p95: 164 Werte ersetzt durch 129\n",
      "  âœ“ DAPI_c13: 6 Werte ersetzt durch 12.8\n",
      "  âœ“ TCRdelta_c13: 138 Werte ersetzt durch 0.927\n",
      "  âœ“ TCRdelta_c13_p95: 188 Werte ersetzt durch 670\n",
      "  âœ“ DAPI_c14: 6 Werte ersetzt durch 107\n",
      "  âœ“ betaCatenin_c14: 110 Werte ersetzt durch 0.792\n",
      "  âœ“ betaCatenin_c14_p95: 157 Werte ersetzt durch 280\n",
      "  âœ“ CD14_c14: 24 Werte ersetzt durch 0.378\n",
      "  âœ“ CD14_c14_p95: 31 Werte ersetzt durch 165\n",
      "  âœ“ DAPI_c15: 6 Werte ersetzt durch 38\n",
      "  âœ“ CD45_c15: 56 Werte ersetzt durch 3.29\n",
      "  âœ“ CD45_c15_p95: 84 Werte ersetzt durch 170\n",
      "  âœ“ CD117_c15: 77 Werte ersetzt durch 1.22\n",
      "  âœ“ CD117_c15_p95: 130 Werte ersetzt durch 523\n",
      "  âœ“ CD34_c16: 152 Werte ersetzt durch 0.531\n",
      "  âœ“ CD34_c16_p95: 252 Werte ersetzt durch 60.3\n",
      "  âœ“ CD19_c16: 156 Werte ersetzt durch 0.084\n",
      "  âœ“ CD19_c16_p95: 251 Werte ersetzt durch 8.95\n",
      "  âœ“ CD127_c16: 156 Werte ersetzt durch 0.15\n",
      "  âœ“ CD127_c16_p95: 251 Werte ersetzt durch 14.9\n",
      "  âœ“ DAPI_c17: 1 Werte ersetzt durch 2.23\n",
      "  âœ“ CD31_c17: 63 Werte ersetzt durch 3\n",
      "  âœ“ CD31_c17_p95: 90 Werte ersetzt durch 231\n",
      "  âœ“ CD11c_c17: 61 Werte ersetzt durch 1.77\n",
      "  âœ“ CD11c_c17_p95: 97 Werte ersetzt durch 103\n",
      "  âœ“ ECadherin_c17: 61 Werte ersetzt durch 0.508\n",
      "  âœ“ ECadherin_c17_p95: 97 Werte ersetzt durch 32.1\n",
      "  âœ“ 70 Spalten stabilisiert\n",
      "--- Stabilisierung abgeschlossen ---\n",
      "\n",
      "INFO: Zielordner fÃ¼r CSV sichergestellt/erstellt: /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/csv\n",
      "Erste Zeilen der Feature-Tabelle ((15380, 86)):\n",
      "   CellID    Area  Y_centroid   X_centroid  MajorAxisLength  MinorAxisLength  \\\n",
      "0       1   935.0   25.432086  1875.482353        51.595367        23.440716   \n",
      "1       2   566.0    6.777385  2106.464664        46.765788        16.774742   \n",
      "2       3  2042.0   17.128795  1961.330558        66.189488        42.236933   \n",
      "3       5  1030.0   33.103883  1894.710680        61.261245        23.216379   \n",
      "4       6  1613.0   32.911345  1919.254185        62.236154        38.170321   \n",
      "\n",
      "   Eccentricity  Solidity  Orientation     DAPI_c3  ...  CD19_c16_p95  \\\n",
      "0      0.890839  0.926660    -0.086526  633.225668  ...         238.0   \n",
      "1      0.933454  0.926350    -1.398578   12.500000  ...         226.0   \n",
      "2      0.769936  0.948004    -1.518392  401.062194  ...         236.0   \n",
      "3      0.925408  0.859766    -0.121628  459.610680  ...         239.0   \n",
      "4      0.789839  0.872835     0.113208  383.334780  ...         234.0   \n",
      "\n",
      "    CD127_c16  CD127_c16_p95    DAPI_c17    CD31_c17  CD31_c17_p95  \\\n",
      "0  355.736898          403.3  451.629947  952.776471       1043.30   \n",
      "1   72.710247          380.0   12.199647   87.803887        759.50   \n",
      "2  356.093046          399.0  354.364349  927.545544       1029.95   \n",
      "3  354.592233          401.0  327.183495  960.585437       1052.55   \n",
      "4  353.928084          393.0  281.154371  951.933664       1044.40   \n",
      "\n",
      "     CD11c_c17  CD11c_c17_p95  ECadherin_c17  ECadherin_c17_p95  \n",
      "0  1070.801070        1176.00     329.349733             382.60  \n",
      "1   112.950530         959.25      37.641343             304.25  \n",
      "2  1063.629775        1164.00     321.611655             374.00  \n",
      "3  1062.166990        1161.00     321.784466             371.00  \n",
      "4  1065.768134        1158.00     322.761934             370.00  \n",
      "\n",
      "[5 rows x 86 columns]\n",
      "Einzelzell-Feature-Tabelle gespeichert unter: /Users/benjaminkramer/Python/Epoxy_CyNif/data/export/sample_191/csv/fused_decon_AF_cleaned.csv\n",
      "Zelle 2.5 (Manuelle Quantifizierung) erfolgreich ausgefÃ¼hrt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zelle 2.5: Manuelle Quantifizierung und Erstellung der CSV-Datei (DYNAMISCH)\n",
    "print(\"--- Zelle 2.5: Manuelle Quantifizierung und Erstellung der CSV-Datei ---\")\n",
    "\n",
    "from skimage.measure import regionprops_table, regionprops\n",
    "import tifffile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ÃœberprÃ¼fe, ob die notwendigen Variablen aus Zelle 2 existieren\n",
    "if 'config_dict_from_yaml' not in locals():\n",
    "    raise NameError(\"config_dict_from_yaml ist nicht definiert. FÃ¼hren Sie Zelle 2 erneut aus.\")\n",
    "if 'all_channel_names_from_markers_csv' not in locals():\n",
    "    raise NameError(\"all_channel_names_from_markers_csv ist nicht definiert. FÃ¼hren Sie Zelle 2 erneut aus.\")\n",
    "if 'SAMPLE_NAME_FOR_FILES_QUANT' not in locals():\n",
    "    raise NameError(\"SAMPLE_NAME_FOR_FILES_QUANT ist nicht definiert. FÃ¼hren Sie Zelle 2 erneut aus.\")\n",
    "\n",
    "# --- DYNAMISCHE DATEIERKENNUNG (wie in Zelle 1) ---\n",
    "def find_first_file(directory, extensions, exclude_patterns=None):\n",
    "    \"\"\"Findet die erste Datei mit passender Endung\"\"\"\n",
    "    if exclude_patterns is None:\n",
    "        exclude_patterns = []\n",
    "    dir_path = BASE_DIR / directory\n",
    "    if not dir_path.exists():\n",
    "        return None\n",
    "    for ext in extensions:\n",
    "        for file in sorted(dir_path.glob(f\"*{ext}\")):\n",
    "            if any(pattern in file.name for pattern in exclude_patterns):\n",
    "                continue\n",
    "            return file\n",
    "    return None\n",
    "\n",
    "print(\"\\n--- Suche vorhandene Dateien fÃ¼r Quantifizierung ---\")\n",
    "\n",
    "# TIF: Suche tatsÃ¤chlich vorhandene Datei\n",
    "tif_file = find_first_file('tif', ['.ome.tif', '.tif'], exclude_patterns=['.tiff'])\n",
    "if not tif_file:\n",
    "    raise FileNotFoundError(f\"FEHLER: Keine TIF-Datei in {BASE_DIR / 'tif'} gefunden!\")\n",
    "path_image_for_quant = tif_file\n",
    "print(f\"Verwende TIF: {path_image_for_quant.name}\")\n",
    "\n",
    "# MASK: Suche tatsÃ¤chlich vorhandene Datei\n",
    "mask_file = find_first_file('mask', ['.tif', '.tiff'], exclude_patterns=[])\n",
    "if not mask_file:\n",
    "    raise FileNotFoundError(f\"FEHLER: Keine Mask-Datei in {BASE_DIR / 'mask'} gefunden!\")\n",
    "path_cell_mask_for_quant = mask_file\n",
    "print(f\"Verwende MASK: {path_cell_mask_for_quant.name}\")\n",
    "\n",
    "# CSV: Ziel fÃ¼r Ausgabe\n",
    "path_csv_output_for_quant = BASE_DIR / 'csv' / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.csv\"\n",
    "print(f\"Ausgabe CSV: {path_csv_output_for_quant.name}\")\n",
    "print(\"--- Dateien gefunden ---\\n\")\n",
    "\n",
    "markers_to_exclude_for_quant = config_dict_from_yaml.get('markersToExclude', [])\n",
    "quantification_channel_names = [\n",
    "    name for name in all_channel_names_from_markers_csv\n",
    "    if name not in markers_to_exclude_for_quant\n",
    "]\n",
    "print(f\"FÃ¼r die Quantifizierung werden {len(quantification_channel_names)} KanÃ¤le verwendet (nach Ausschluss).\")\n",
    "print(f\"  Ausgeschlossene Marker fÃ¼r Quant (aus config): {markers_to_exclude_for_quant[:5]}... (bis zu 5 gezeigt)\")\n",
    "print(f\"  Zu quantifizierende Marker: {quantification_channel_names[:5]}... (bis zu 5 gezeigt)\")\n",
    "\n",
    "# --- Bild und Maske laden ---\n",
    "print(f\"Lade Bild fÃ¼r Quantifizierung: {path_image_for_quant}\")\n",
    "if not path_image_for_quant.exists():\n",
    "    raise FileNotFoundError(f\"Bilddatei nicht gefunden: {path_image_for_quant}\")\n",
    "full_image_data = tifffile.imread(path_image_for_quant)\n",
    "print(f\"  Bildform roh: {full_image_data.shape}\")\n",
    "\n",
    "# Bilddaten in (H, W, C) bringen, falls nÃ¶tig (Logik von Ihnen Ã¼bernommen)\n",
    "if full_image_data.ndim == 3 and full_image_data.shape[0] == len(all_channel_names_from_markers_csv):\n",
    "    full_image_data_hwc = np.transpose(full_image_data, (1, 2, 0))\n",
    "elif full_image_data.ndim == 3 and full_image_data.shape[2] == len(all_channel_names_from_markers_csv):\n",
    "    full_image_data_hwc = full_image_data\n",
    "elif full_image_data.ndim == 2 and len(all_channel_names_from_markers_csv) == 1:\n",
    "    full_image_data_hwc = np.expand_dims(full_image_data, axis=-1)\n",
    "else:\n",
    "    raise ValueError(f\"Unerwartete Bilddimensionen {full_image_data.shape} fÃ¼r {len(all_channel_names_from_markers_csv)} KanÃ¤le.\")\n",
    "print(f\"  Bildform fÃ¼r Quantifizierung (H,W,C): {full_image_data_hwc.shape}\")\n",
    "\n",
    "print(f\"Lade Zellmaske fÃ¼r Quantifizierung: {path_cell_mask_for_quant}\")\n",
    "if not path_cell_mask_for_quant.exists():\n",
    "    raise FileNotFoundError(f\"Zellmaskendatei nicht gefunden: {path_cell_mask_for_quant}\")\n",
    "cell_mask = tifffile.imread(path_cell_mask_for_quant) # WICHTIG: cell_mask wird hier definiert\n",
    "print(f\"  Zellmaskenform: {cell_mask.shape}, Max Label: {np.max(cell_mask)}\")\n",
    "\n",
    "# WICHTIG: Validiere dass Image und Maske kompatible Shapes haben\n",
    "if cell_mask.shape[:2] != full_image_data_hwc.shape[:2]:\n",
    "    print(f\"\\nâš ï¸  WARNUNG: Shape-Mismatch!\")\n",
    "    print(f\"   Image (H,W,C): {full_image_data_hwc.shape}\")\n",
    "    print(f\"   Mask  (H,W):   {cell_mask.shape}\")\n",
    "    print(f\"\\n   â†’ Passe Maske an Image-Dimensionen an...\")\n",
    "    \n",
    "    from skimage.transform import resize\n",
    "    # Resize Maske auf Image-Dimensionen (nearest neighbor fÃ¼r Labels!)\n",
    "    cell_mask_resized = resize(\n",
    "        cell_mask.astype(float), \n",
    "        full_image_data_hwc.shape[:2], \n",
    "        order=0,  # Nearest neighbor (wichtig fÃ¼r Labels!)\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=False\n",
    "    ).astype(cell_mask.dtype)\n",
    "    cell_mask = cell_mask_resized\n",
    "    print(f\"   âœ“ Maske angepasst auf: {cell_mask.shape}\")\n",
    "\n",
    "# --- DataFrame Erstellung und Quantifizierung ---\n",
    "if np.max(cell_mask) == 0:\n",
    "    print(\"WARNUNG: Zellmaske enthÃ¤lt keine Labels (max Label = 0). Erstelle leere CSV.\")\n",
    "    df_features = pd.DataFrame() # df_features wird hier fÃ¼r den leeren Fall definiert\n",
    "else:\n",
    "    print(\"Berechne morphologische Eigenschaften...\")\n",
    "    features_table = regionprops_table(\n",
    "        cell_mask,\n",
    "        properties=('label', 'area', 'centroid', 'major_axis_length',\n",
    "                    'minor_axis_length', 'eccentricity', 'solidity', 'orientation')\n",
    "    )\n",
    "    df_features = pd.DataFrame(features_table) # df_features wird hier fÃ¼r den nicht-leeren Fall definiert\n",
    "    print(f\"  Spalten direkt nach regionprops_table: {df_features.columns.tolist()}\")\n",
    "\n",
    "    df_features.rename(columns={\n",
    "        'label': 'CellID', 'area': 'Area', 'centroid-0': 'Y_centroid', 'centroid-1': 'X_centroid',\n",
    "        'major_axis_length': 'MajorAxisLength', 'minor_axis_length': 'MinorAxisLength',\n",
    "        'eccentricity': 'Eccentricity', 'solidity': 'Solidity', 'orientation': 'Orientation'\n",
    "    }, inplace=True)\n",
    "    print(f\"  Spalten nach Umbenennung: {df_features.columns.tolist()}\")\n",
    "\n",
    "    if 'Area' in df_features.columns:\n",
    "        df_features['Area'] = df_features['Area'].astype(float)\n",
    "    else:\n",
    "        print(\"WARNUNG: Spalte 'Area' nach Umbenennung immer noch nicht gefunden!\")\n",
    "\n",
    "    print(f\"\\nExtrahiere IntensitÃ¤ten fÃ¼r {len(quantification_channel_names)} ausgewÃ¤hlte Marker pro Zelle...\")\n",
    "    print(\"  Strategie: Mean (Standard) + 95th Percentile (fÃ¼r Membranmarker)\")\n",
    "    original_channel_name_to_index = {name: i for i, name in enumerate(all_channel_names_from_markers_csv)}\n",
    "    \n",
    "    # Lade markers.csv fÃ¼r localization Info\n",
    "    markers_df_loc = pd.read_csv(MARKERS_CSV_PATH)\n",
    "    marker_localization = dict(zip(markers_df_loc['marker_name'], markers_df_loc.get('localization', ['unknown']*len(markers_df_loc))))\n",
    "    \n",
    "    # PrÃ¼fe ob quantificationMetrics in config vorhanden\n",
    "    compute_p95 = config_dict_from_yaml.get('quantificationMetrics', {}).get('compute_percentile_95', False)\n",
    "    \n",
    "    # Progress tracking\n",
    "    from tqdm.auto import tqdm\n",
    "    membrane_count = 0\n",
    "    progress_bar = tqdm(quantification_channel_names, desc=\"Quantifiziere Marker\", unit=\"marker\")\n",
    "    \n",
    "    for marker_name_to_quantify in progress_bar:\n",
    "        if marker_name_to_quantify not in original_channel_name_to_index:\n",
    "            print(f\"    WARNUNG: Zu quantifizierender Marker '{marker_name_to_quantify}' nicht in ursprÃ¼nglichen Kanalnamen gefunden. Ãœberspringe.\")\n",
    "            if not df_features.empty: df_features[marker_name_to_quantify] = np.nan # Nur hinzufÃ¼gen, wenn df nicht leer\n",
    "            continue\n",
    "\n",
    "        channel_index = original_channel_name_to_index[marker_name_to_quantify]\n",
    "        is_membrane = marker_localization.get(marker_name_to_quantify, 'unknown') == 'membrane'\n",
    "\n",
    "        if channel_index < full_image_data_hwc.shape[2]:\n",
    "            current_channel_image = full_image_data_hwc[:, :, channel_index]\n",
    "            \n",
    "            # Berechne MEAN (Standard fÃ¼r alle Marker)\n",
    "            intensity_props = regionprops_table(label_image=cell_mask, intensity_image=current_channel_image,\n",
    "                                                properties=['label', 'intensity_mean'])\n",
    "            df_intensity_marker = pd.DataFrame(intensity_props)\n",
    "            df_intensity_marker.rename(columns={'label': 'CellID', 'intensity_mean': marker_name_to_quantify}, inplace=True)\n",
    "            \n",
    "            # Berechne 95th PERCENTILE fÃ¼r Membranmarker (wenn aktiviert)\n",
    "            if compute_p95 and is_membrane:\n",
    "                membrane_count += 1\n",
    "                progress_bar.set_postfix({\"Marker\": marker_name_to_quantify[:20], \"Type\": \"membrane+p95\"})\n",
    "                \n",
    "                # OPTIMIERTE METHODE: ~5-10x schneller als Loop\n",
    "                from scipy.ndimage import labeled_comprehension\n",
    "                cell_ids = df_intensity_marker['CellID'].values\n",
    "                p95_values = labeled_comprehension(\n",
    "                    current_channel_image,\n",
    "                    cell_mask,\n",
    "                    cell_ids,\n",
    "                    lambda pixels: np.percentile(pixels, 95) if len(pixels) > 0 else np.nan,\n",
    "                    float,\n",
    "                    np.nan\n",
    "                )\n",
    "                df_intensity_marker[f'{marker_name_to_quantify}_p95'] = p95_values\n",
    "            else:\n",
    "                progress_bar.set_postfix({\"Marker\": marker_name_to_quantify[:20], \"Type\": \"mean-only\"})\n",
    "            \n",
    "            # Merge mit Haupttabelle\n",
    "            if df_features.empty and not df_intensity_marker.empty: # Wenn df_features leer war, aber jetzt IntensitÃ¤ten da sind\n",
    "                df_features = df_intensity_marker\n",
    "            elif not df_features.empty and not df_intensity_marker.empty :\n",
    "                 df_features = pd.merge(df_features, df_intensity_marker, on='CellID', how='left')\n",
    "            elif not df_intensity_marker.empty : # Sollte nicht passieren, wenn df_features leer und Maske leer war\n",
    "                 print(f\"WARNUNG: df_features ist leer, aber IntensitÃ¤tsdaten fÃ¼r {marker_name_to_quantify} vorhanden. Merging nicht mÃ¶glich.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"    WARNUNG: Kanalindex {channel_index} fÃ¼r Marker '{marker_name_to_quantify}' ist auÃŸerhalb der Bilddimensionen \"\n",
    "                  f\"(max Index ist {full_image_data_hwc.shape[2]-1}). Ãœberspringe IntensitÃ¤tsmessung.\")\n",
    "            if not df_features.empty: df_features[marker_name_to_quantify] = np.nan\n",
    "    \n",
    "    if compute_p95:\n",
    "        print(f\"  âœ“ {membrane_count} Membranmarker bekamen zusÃ¤tzlich _p95 Spalten\")\n",
    "\n",
    "# --- WICHTIG: Stabilisiere ALLE IntensitÃ¤tswerte (Cylinter macht log-Transformationen) ---\n",
    "print(\"\\n--- Stabilisiere IntensitÃ¤tswerte (ersetze â‰¤0 mit Minimum) ---\")\n",
    "intensity_cols = [\n",
    "    col for col in df_features.columns\n",
    "    if isinstance(col, str) and ('_c' in col or 'intensity' in col.lower())\n",
    "]\n",
    "\n",
    "stabilized_count = 0\n",
    "for col in intensity_cols:\n",
    "    series = df_features[col].astype(float)\n",
    "    \n",
    "    # Finde Minimalwert aller positiven Werte\n",
    "    positive = series[series > 0]\n",
    "    if not positive.empty:\n",
    "        floor = positive.min()\n",
    "    else:\n",
    "        floor = 1.0\n",
    "    \n",
    "    # Ersetze alle Null-/Negativwerte\n",
    "    replaced_mask = series <= 0\n",
    "    if replaced_mask.any():\n",
    "        df_features.loc[replaced_mask, col] = floor\n",
    "        stabilized_count += 1\n",
    "        print(f\"  âœ“ {col}: {replaced_mask.sum()} Werte ersetzt durch {floor:.3g}\")\n",
    "    \n",
    "    # ZusÃ¤tzlich: Ersetze NaN/inf Werte\n",
    "    invalid_mask = ~np.isfinite(df_features[col])\n",
    "    if invalid_mask.any():\n",
    "        df_features.loc[invalid_mask, col] = floor\n",
    "        print(f\"  âœ“ {col}: {invalid_mask.sum()} NaN/inf Werte ersetzt\")\n",
    "\n",
    "if stabilized_count == 0:\n",
    "    print(\"  âœ“ Alle IntensitÃ¤tswerte bereits positiv (keine Stabilisierung nÃ¶tig)\")\n",
    "else:\n",
    "    print(f\"  âœ“ {stabilized_count} Spalten stabilisiert\")\n",
    "print(\"--- Stabilisierung abgeschlossen ---\\n\")\n",
    "\n",
    "# --- Sicherstellen, dass der Zielordner fÃ¼r die CSV existiert ---\n",
    "if 'path_csv_output_for_quant' not in locals(): # Sollte durch Zelle 2 definiert sein\n",
    "     raise NameError(\"Variable 'path_csv_output_for_quant' ist nicht definiert.\")\n",
    "\n",
    "csv_output_directory = path_csv_output_for_quant.parent\n",
    "try:\n",
    "    csv_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"INFO: Zielordner fÃ¼r CSV sichergestellt/erstellt: {csv_output_directory}\")\n",
    "except Exception as e_mkdir:\n",
    "    print(f\"FEHLER beim Erstellen des CSV-Zielordners {csv_output_directory}: {e_mkdir}\")\n",
    "    raise\n",
    "# --- Ende Ordnererstellung ---\n",
    "\n",
    "# Sicherstellen, dass 'CellID' die erste Spalte ist, falls sie existiert und df nicht leer ist\n",
    "if not df_features.empty and 'CellID' in df_features.columns:\n",
    "    cols = ['CellID'] + [col for col in df_features.columns if col != 'CellID']\n",
    "    df_features = df_features[cols]\n",
    "elif df_features.empty:\n",
    "    print(\"INFO: df_features ist leer. Es wird eine leere CSV-Datei gespeichert.\")\n",
    "else: # df_features nicht leer, aber 'CellID' fehlt\n",
    "    print(\"WARNUNG: Spalte 'CellID' fehlt im finalen DataFrame, obwohl Daten vorhanden sind.\")\n",
    "\n",
    "\n",
    "print(f\"Erste Zeilen der Feature-Tabelle ({df_features.shape}):\\n{df_features.head()}\")\n",
    "df_features.to_csv(path_csv_output_for_quant, index=False)\n",
    "print(f\"Einzelzell-Feature-Tabelle gespeichert unter: {path_csv_output_for_quant}\")\n",
    "print(\"Zelle 2.5 (Manuelle Quantifizierung) erfolgreich ausgefÃ¼hrt.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f094ddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRE-FLIGHT CHECK: ÃœberprÃ¼fe alle Voraussetzungen fÃ¼r CyLinter\n",
      "================================================================================\n",
      "\n",
      "[1] ÃœberprÃ¼fe benÃ¶tigte Variablen aus vorherigen Zellen...\n",
      "  âœ“ BASE_DIR vorhanden\n",
      "  âœ“ CONFIG_PATH vorhanden\n",
      "  âœ“ MARKERS_CSV_PATH vorhanden\n",
      "  âœ“ all_channel_names_from_markers_csv vorhanden\n",
      "  âœ“ SAMPLE_NAME_FOR_FILES_QUANT vorhanden\n",
      "\n",
      "[2] ÃœberprÃ¼fe markers.csv Format...\n",
      "  âœ“ markers.csv geladen: 76 EintrÃ¤ge\n",
      "  âœ“ Marker-Namen haben _cX (korrekt fÃ¼r aggregateData): ['DAPI_c3', 'AF1_c3', 'AF2_c3']\n",
      "  âœ“ Keine Leerzeichen in Marker-Namen\n",
      "\n",
      "[3] ÃœberprÃ¼fe cylinter_config.yml...\n",
      "  âœ“ Config geladen\n",
      "  âœ“ sampleMetadata korrekt: fused_decon_AF_cleaned -> 5-Element Liste\n",
      "  âœ“ counterstainChannel 'DAPI_c3' in markers.csv gefunden\n",
      "  âœ“ samplesForROISelection stimmt mit sampleMetadata Ã¼berein: 'fused_decon_AF_cleaned'\n",
      "\n",
      "[4] ÃœberprÃ¼fe erstellte CSV-Datei...\n",
      "  âœ“ CSV gefunden: fused_decon_AF_cleaned.csv\n",
      "    Zeilen: 15380, Spalten: 86\n",
      "  âœ“ CellID Spalte vorhanden\n",
      "  âš  WARNUNG: 29 unerwartete Marker in CSV\n",
      "     Erste 3: ['Epcam_c6_p95', 'MLH1_c5_p95', 'ECadherin_c17_p95']\n",
      "\n",
      "[5] ÃœberprÃ¼fe DateizÃ¤hlung in Ordnern...\n",
      "  âœ“ Alle Ordner haben gleiche Anzahl Dateien: {'csv': 1, 'tif': 1, 'seg': 1, 'mask': 1}\n",
      "\n",
      "================================================================================\n",
      "ZUSAMMENFASSUNG PRE-FLIGHT CHECK\n",
      "================================================================================\n",
      "âœ“ Erfolgreich: 15\n",
      "âœ— Fehler:      0\n",
      "âš  Warnungen:   1\n",
      "\n",
      "ðŸŽ‰ ALLE CHECKS BESTANDEN! CyLinter kann gestartet werden (Zelle 8).\n",
      "\n",
      "NÃ„CHSTE SCHRITTE:\n",
      "  1. FÃ¼hren Sie Zelle 8 aus (Konfigurations-GUI)\n",
      "  2. FÃ¼hren Sie Zelle 9 aus (Pipeline-Start)\n",
      "================================================================================\n",
      "Pre-Flight Check abgeschlossen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zelle 2.75: PRE-FLIGHT CHECK - ÃœberprÃ¼fe alle Bedingungen vor CyLinter Start\n",
    "print(\"=\" * 80)\n",
    "print(\"PRE-FLIGHT CHECK: ÃœberprÃ¼fe alle Voraussetzungen fÃ¼r CyLinter\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# ZÃ¤hler fÃ¼r Erfolge und Fehler\n",
    "checks_passed = 0\n",
    "checks_failed = 0\n",
    "warnings = 0\n",
    "\n",
    "# --- CHECK 1: BenÃ¶tigte Variablen ---\n",
    "print(\"\\n[1] ÃœberprÃ¼fe benÃ¶tigte Variablen aus vorherigen Zellen...\")\n",
    "required_vars = ['BASE_DIR', 'CONFIG_PATH', 'MARKERS_CSV_PATH', 'all_channel_names_from_markers_csv', 'SAMPLE_NAME_FOR_FILES_QUANT']\n",
    "for var_name in required_vars:\n",
    "    if var_name in locals() or var_name in globals():\n",
    "        print(f\"  âœ“ {var_name} vorhanden\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"  âœ— FEHLER: {var_name} nicht gefunden! FÃ¼hren Sie vorherige Zellen aus.\")\n",
    "        checks_failed += 1\n",
    "\n",
    "# --- CHECK 2: markers.csv Format ---\n",
    "print(\"\\n[2] ÃœberprÃ¼fe markers.csv Format...\")\n",
    "try:\n",
    "    df_markers_check = pd.read_csv(MARKERS_CSV_PATH)\n",
    "    print(f\"  âœ“ markers.csv geladen: {len(df_markers_check)} EintrÃ¤ge\")\n",
    "    checks_passed += 1\n",
    "    \n",
    "    # WICHTIG: PrÃ¼fe dass markers.csv _cX Suffixe hat (fÃ¼r aggregateData)\n",
    "    sample_markers = df_markers_check['marker_name'].head(5).tolist()\n",
    "    has_cycle_suffix = any('_c' in str(m) for m in sample_markers)\n",
    "    \n",
    "    if not has_cycle_suffix:\n",
    "        print(f\"  âœ— FEHLER: Marker-Namen haben KEINE _cX Suffix: {sample_markers[:3]}\")\n",
    "        print(f\"     â†’ markers.csv sollte _cX haben (fÃ¼r aggregateData CSV-Matching)\")\n",
    "        print(f\"     â†’ Zelle 1 sollte dies automatisch korrigieren\")\n",
    "        checks_failed += 1\n",
    "    else:\n",
    "        print(f\"  âœ“ Marker-Namen haben _cX (korrekt fÃ¼r aggregateData): {sample_markers[:3]}\")\n",
    "        checks_passed += 1\n",
    "    \n",
    "    # PrÃ¼fe auf Leerzeichen\n",
    "    has_spaces = any(' ' in str(m) for m in df_markers_check['marker_name'])\n",
    "    if has_spaces:\n",
    "        print(f\"  âš  WARNUNG: Leerzeichen in Marker-Namen gefunden\")\n",
    "        print(f\"     â†’ Zelle 1 sollte diese entfernen\")\n",
    "        warnings += 1\n",
    "    else:\n",
    "        print(f\"  âœ“ Keine Leerzeichen in Marker-Namen\")\n",
    "        checks_passed += 1\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  âœ— FEHLER beim Laden: {e}\")\n",
    "    checks_failed += 1\n",
    "\n",
    "# --- CHECK 3: cylinter_config.yml ---\n",
    "print(\"\\n[3] ÃœberprÃ¼fe cylinter_config.yml...\")\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"  âœ“ Config geladen\")\n",
    "    checks_passed += 1\n",
    "    \n",
    "    # PrÃ¼fe sampleMetadata Format\n",
    "    smd = config.get('sampleMetadata', {})\n",
    "    if smd:\n",
    "        sample_key = list(smd.keys())[0]\n",
    "        sample_value = smd[sample_key]\n",
    "        if isinstance(sample_value, list) and len(sample_value) == 5:\n",
    "            print(f\"  âœ“ sampleMetadata korrekt: {sample_key} -> 5-Element Liste\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  âœ— FEHLER: sampleMetadata Format falsch: {sample_value}\")\n",
    "            print(f\"     â†’ Muss 5-Element Liste sein: [name, condition, replicate, group, order]\")\n",
    "            checks_failed += 1\n",
    "    else:\n",
    "        print(f\"  âœ— FEHLER: sampleMetadata fehlt\")\n",
    "        checks_failed += 1\n",
    "    \n",
    "    # PrÃ¼fe counterstainChannel\n",
    "    counterstain = config.get('counterstainChannel')\n",
    "    if counterstain:\n",
    "        if counterstain in df_markers_check['marker_name'].values:\n",
    "            print(f\"  âœ“ counterstainChannel '{counterstain}' in markers.csv gefunden\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  âœ— FEHLER: counterstainChannel '{counterstain}' NICHT in markers.csv!\")\n",
    "            print(f\"     â†’ VerfÃ¼gbare DAPI Marker: {[m for m in df_markers_check['marker_name'] if 'DAPI' in str(m)][:3]}\")\n",
    "            checks_failed += 1\n",
    "    else:\n",
    "        print(f\"  âœ— FEHLER: counterstainChannel fehlt in Config\")\n",
    "        checks_failed += 1\n",
    "    \n",
    "    # PrÃ¼fe samplesForROISelection\n",
    "    roi_samples = config.get('samplesForROISelection', [])\n",
    "    if roi_samples:\n",
    "        roi_sample_name = roi_samples[0] if roi_samples else None\n",
    "        if roi_sample_name == sample_key:\n",
    "            print(f\"  âœ“ samplesForROISelection stimmt mit sampleMetadata Ã¼berein: '{roi_sample_name}'\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  âœ— FEHLER: samplesForROISelection '{roi_sample_name}' != sampleMetadata '{sample_key}'\")\n",
    "            checks_failed += 1\n",
    "    else:\n",
    "        print(f\"  âš  WARNUNG: samplesForROISelection ist leer\")\n",
    "        warnings += 1\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  âœ— FEHLER beim Laden: {e}\")\n",
    "    checks_failed += 1\n",
    "\n",
    "# --- CHECK 4: CSV Datei ---\n",
    "print(\"\\n[4] ÃœberprÃ¼fe erstellte CSV-Datei...\")\n",
    "try:\n",
    "    csv_path = BASE_DIR / 'csv' / f\"{SAMPLE_NAME_FOR_FILES_QUANT}.csv\"\n",
    "    if csv_path.exists():\n",
    "        df_csv = pd.read_csv(csv_path)\n",
    "        print(f\"  âœ“ CSV gefunden: {csv_path.name}\")\n",
    "        print(f\"    Zeilen: {len(df_csv)}, Spalten: {len(df_csv.columns)}\")\n",
    "        checks_passed += 1\n",
    "        \n",
    "        # PrÃ¼fe CellID\n",
    "        if 'CellID' in df_csv.columns:\n",
    "            print(f\"  âœ“ CellID Spalte vorhanden\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"  âœ— FEHLER: CellID Spalte fehlt!\")\n",
    "            checks_failed += 1\n",
    "        \n",
    "        # KORRIGIERT: Vergleiche nur mit NICHT-EXCLUDIERTEN Markern!\n",
    "        csv_marker_cols = [col for col in df_csv.columns if col not in ['CellID', 'Area', 'X_centroid', 'Y_centroid', \n",
    "                                                                          'MajorAxisLength', 'MinorAxisLength', \n",
    "                                                                          'Eccentricity', 'Solidity', 'Orientation']]\n",
    "        \n",
    "        # Hole excludierte Marker aus Config\n",
    "        excluded_markers = config.get('markersToExclude', [])\n",
    "        expected_markers = set(df_markers_check['marker_name'].values) - set(excluded_markers)\n",
    "        csv_marker_set = set(csv_marker_cols)\n",
    "        \n",
    "        if csv_marker_set == expected_markers:\n",
    "            print(f\"  âœ“ CSV Spalten stimmen mit erwarteten Markern Ã¼berein ({len(csv_marker_set)} Marker)\")\n",
    "            print(f\"    ({len(excluded_markers)} Marker ausgeschlossen wie konfiguriert)\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            missing_in_csv = expected_markers - csv_marker_set\n",
    "            extra_in_csv = csv_marker_set - expected_markers\n",
    "            if missing_in_csv:\n",
    "                print(f\"  âœ— FEHLER: {len(missing_in_csv)} erwartete Marker fehlen in CSV\")\n",
    "                print(f\"     Erste 3: {list(missing_in_csv)[:3]}\")\n",
    "                checks_failed += 1\n",
    "            if extra_in_csv:\n",
    "                print(f\"  âš  WARNUNG: {len(extra_in_csv)} unerwartete Marker in CSV\")\n",
    "                print(f\"     Erste 3: {list(extra_in_csv)[:3]}\")\n",
    "                warnings += 1\n",
    "            if not missing_in_csv and not extra_in_csv:\n",
    "                checks_passed += 1\n",
    "                \n",
    "    else:\n",
    "        print(f\"  âœ— FEHLER: CSV nicht gefunden: {csv_path}\")\n",
    "        print(f\"     â†’ FÃ¼hren Sie Zelle 2.5 aus\")\n",
    "        checks_failed += 1\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  âœ— FEHLER: {e}\")\n",
    "    checks_failed += 1\n",
    "\n",
    "# --- CHECK 5: DateizÃ¤hlung (KORRIGIERT: ignoriere .DS_Store) ---\n",
    "print(\"\\n[5] ÃœberprÃ¼fe DateizÃ¤hlung in Ordnern...\")\n",
    "folders_to_check = ['csv', 'tif', 'seg', 'mask']\n",
    "file_counts = {}\n",
    "for folder in folders_to_check:\n",
    "    folder_path = BASE_DIR / folder\n",
    "    if folder_path.exists():\n",
    "        # Ignoriere .DS_Store und andere versteckte Dateien\n",
    "        files = [f for f in folder_path.glob('*') if f.is_file() and not f.name.startswith('.')]\n",
    "        file_counts[folder] = len(files)\n",
    "    else:\n",
    "        file_counts[folder] = 0\n",
    "\n",
    "all_counts_equal = len(set(file_counts.values())) == 1\n",
    "if all_counts_equal and file_counts['csv'] > 0:\n",
    "    print(f\"  âœ“ Alle Ordner haben gleiche Anzahl Dateien: {file_counts}\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"  âœ— FEHLER: Unterschiedliche Dateianzahl: {file_counts}\")\n",
    "    print(f\"     â†’ Alle Ordner mÃ¼ssen GENAU 1 Datei haben mit gleichem Namen\")\n",
    "    checks_failed += 1\n",
    "\n",
    "# --- ZUSAMMENFASSUNG ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ZUSAMMENFASSUNG PRE-FLIGHT CHECK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ“ Erfolgreich: {checks_passed}\")\n",
    "print(f\"âœ— Fehler:      {checks_failed}\")\n",
    "print(f\"âš  Warnungen:   {warnings}\")\n",
    "print()\n",
    "\n",
    "if checks_failed == 0:\n",
    "    print(\"ðŸŽ‰ ALLE CHECKS BESTANDEN! CyLinter kann gestartet werden (Zelle 8).\")\n",
    "    print()\n",
    "    print(\"NÃ„CHSTE SCHRITTE:\")\n",
    "    print(\"  1. FÃ¼hren Sie Zelle 8 aus (Konfigurations-GUI)\")\n",
    "    print(\"  2. FÃ¼hren Sie Zelle 9 aus (Pipeline-Start)\")\n",
    "else:\n",
    "    print(\"âŒ FEHLER GEFUNDEN! Bitte beheben Sie die Probleme vor CyLinter Start.\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Pre-Flight Check abgeschlossen.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a7525",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ?? Zelle 8: Konfigurations-GUI (Tag 1)\n",
    "\n",
    "Marker-Subset ausw?hlen (Schnellauswahl oder Custom). Speichert nur `markersToExclude` in `cylinter_config.yml`; Pipeline startet in Zelle 9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8db0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KONFIGURATIONS-GUI (TAG 1 - ERSTE ANALYSE)\n",
      "================================================================================\n",
      "Marker geladen: 14 DAPI, 28 AF, 34 Bio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e17b0a7f6af402f94c56e6eef591fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Marker-Konfiguration</h3>'), HTML(value='\\n<div style=\"background-color: #fff3câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Konfigurations-GUI bereit!\n",
      "   Waehlen Sie Marker-Strategie und klicken Sie 'Speichern & Bereit'\n",
      "   Pipeline wird dann in Zelle 9 gestartet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zelle 8: Konfigurations-GUI (TAG 1 - Erste Analyse)\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"KONFIGURATIONS-GUI (TAG 1 - ERSTE ANALYSE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ÃœberprÃ¼fe benÃ¶tigte Variablen\n",
    "if 'BASE_DIR' not in locals():\n",
    "    BASE_DIR = Path.cwd()\n",
    "if 'CONFIG_PATH' not in locals():\n",
    "    CONFIG_PATH = BASE_DIR / \"cylinter_config.yml\"\n",
    "if 'MARKERS_CSV_PATH' not in locals():\n",
    "    MARKERS_CSV_PATH = BASE_DIR / \"markers.csv\"\n",
    "\n",
    "# Lade markers.csv\n",
    "df_markers = pd.read_csv(MARKERS_CSV_PATH)\n",
    "\n",
    "# Kategorisiere Marker\n",
    "dapi_markers = df_markers[df_markers['marker_name'].str.contains('DAPI', case=False, na=False)]['marker_name'].tolist()\n",
    "af_markers = df_markers[df_markers['marker_name'].str.contains('AF[12]_', case=True, na=False, regex=True)]['marker_name'].tolist()\n",
    "bio_markers = df_markers[\n",
    "    ~df_markers['marker_name'].str.contains('DAPI', case=False, na=False) &\n",
    "    ~df_markers['marker_name'].str.contains('AF[12]_', case=True, na=False, regex=True)\n",
    "]['marker_name'].tolist()\n",
    "\n",
    "print(f\"Marker geladen: {len(dapi_markers)} DAPI, {len(af_markers)} AF, {len(bio_markers)} Bio\")\n",
    "\n",
    "# Quick Options\n",
    "quick_label = widgets.HTML(\"<h4>Schnellauswahl:</h4>\")\n",
    "quick_option = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('Alle 96 Marker verwenden', 'all'),\n",
    "        ('Alle ausser AF (58 Marker: DAPI + Bio)', 'no_af'),\n",
    "        ('Nur Bio-Marker (ohne DAPI/AF)', 'bio_only'),\n",
    "        ('Custom (unten auswaehlen)', 'custom')\n",
    "    ],\n",
    "    value='no_af',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "quick_info = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #e3f2fd; padding: 10px; border-left: 4px solid #2196F3; margin: 10px 0;\">\n",
    "<b>Empfehlung:</b> \"Alle ausser AF\" (58 Marker)<br>\n",
    "Autofluoreszenz-Marker werden automatisch ausgeschlossen.\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Bio-Marker Checkboxen\n",
    "bio_checkboxes = {}\n",
    "bio_checkbox_widgets = []\n",
    "\n",
    "for marker in sorted(bio_markers):\n",
    "    cb = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description=marker,\n",
    "        indent=False,\n",
    "        layout=widgets.Layout(width='220px'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    bio_checkboxes[marker] = cb\n",
    "    bio_checkbox_widgets.append(cb)\n",
    "\n",
    "bio_grid = widgets.GridBox(bio_checkbox_widgets, layout=widgets.Layout(\n",
    "    grid_template_columns='repeat(3, 220px)',\n",
    "    grid_gap='5px',\n",
    "    margin='10px 0',\n",
    "    max_height='400px',\n",
    "    overflow_y='auto'\n",
    "))\n",
    "\n",
    "# Bio-Marker Controls\n",
    "bio_select_all = widgets.Button(description='Alle', layout=widgets.Layout(width='120px'))\n",
    "bio_deselect_all = widgets.Button(description='Keine', layout=widgets.Layout(width='120px'))\n",
    "\n",
    "def bio_select_all_click(_):\n",
    "    for cb in bio_checkboxes.values():\n",
    "        cb.value = True\n",
    "\n",
    "def bio_deselect_all_click(_):\n",
    "    for cb in bio_checkboxes.values():\n",
    "        cb.value = False\n",
    "\n",
    "bio_select_all.on_click(bio_select_all_click)\n",
    "bio_deselect_all.on_click(bio_deselect_all_click)\n",
    "\n",
    "bio_controls = widgets.HBox([bio_select_all, bio_deselect_all])\n",
    "detail_label = widgets.HTML(\"<h4>Custom: Bio-Marker einzeln auswaehlen</h4>\")\n",
    "\n",
    "# Status Output\n",
    "status_output = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ccc', padding='10px', margin='10px 0',\n",
    "    max_height='300px', overflow_y='auto'\n",
    "))\n",
    "\n",
    "# Save Function\n",
    "def save_and_ready(b):\n",
    "    \"\"\"Speichert nur Marker-Auswahl - Pipeline wird in Zelle 9 gestartet\"\"\"\n",
    "    with status_output:\n",
    "        status_output.clear_output()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SPEICHERE MARKER-KONFIGURATION\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Marker-Auswahl\n",
    "        option = quick_option.value\n",
    "        markers_to_exclude = []\n",
    "        \n",
    "        if option == 'all':\n",
    "            markers_to_exclude = []\n",
    "            marker_desc = \"Alle 96 Marker\"\n",
    "        elif option == 'no_af':\n",
    "            markers_to_exclude = af_markers\n",
    "            marker_desc = f\"58 Marker (DAPI + Bio, ohne {len(af_markers)} AF)\"\n",
    "        elif option == 'bio_only':\n",
    "            markers_to_exclude = dapi_markers + af_markers\n",
    "            marker_desc = f\"{len(bio_markers)} Bio-Marker (ohne DAPI/AF)\"\n",
    "        elif option == 'custom':\n",
    "            excluded_bio = [m for m, cb in bio_checkboxes.items() if not cb.value]\n",
    "            markers_to_exclude = af_markers + excluded_bio\n",
    "            marker_desc = f\"{len(dapi_markers) + len(bio_markers) - len(excluded_bio)} Marker (Custom)\"\n",
    "        \n",
    "        print(f\"Marker-Auswahl: {marker_desc}\")\n",
    "        print(f\"   Ausgeschlossen: {len(markers_to_exclude)} Marker\")\n",
    "        print(f\"   Werden verwendet: {96 - len(markers_to_exclude)} Marker\")\n",
    "        \n",
    "        # Schreibe Config\n",
    "        try:\n",
    "            # Backup\n",
    "            backup_path = CONFIG_PATH.with_suffix('.yml.backup')\n",
    "            import shutil\n",
    "            shutil.copy2(CONFIG_PATH, backup_path)\n",
    "            print(f\"\\nBackup erstellt: {backup_path.name}\")\n",
    "            \n",
    "            # Lade Config\n",
    "            with open(CONFIG_PATH, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            \n",
    "            # Update markersToExclude\n",
    "            config['markersToExclude'] = markers_to_exclude\n",
    "            \n",
    "            # Schreibe Config\n",
    "            temp_path = CONFIG_PATH.with_suffix('.yml.tmp')\n",
    "            with open(temp_path, 'w') as f:\n",
    "                yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "            temp_path.replace(CONFIG_PATH)\n",
    "            \n",
    "            print(f\"cylinter_config.yml aktualisiert\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"KONFIGURATION GESPEICHERT!\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "            \n",
    "            print(\"NAECHSTER SCHRITT:\")\n",
    "            print(\"   Fuehren Sie ZELLE 9 aus, um die Pipeline zu starten!\")\n",
    "            print(\"   Die Pipeline laeuft von aggregateData bis zum Ende durch.\")\n",
    "            print(\"   Module mit existierenden Checkpoints werden uebersprungen.\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nFEHLER: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "save_btn = widgets.Button(\n",
    "    description='Speichern & Bereit',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='220px', height='45px'),\n",
    "    icon='check',\n",
    "    tooltip='Speichert Marker-Konfiguration (startet KEINE Pipeline!)'\n",
    ")\n",
    "save_btn.on_click(save_and_ready)\n",
    "\n",
    "# UI\n",
    "info_box = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 15px; border-left: 4px solid #ffc107; margin-bottom: 15px;\">\n",
    "<b>Funktionsweise:</b><br>\n",
    "Diese GUI konfiguriert nur die <b>Marker-Auswahl</b><br>\n",
    "Die Pipeline startet in <b>Zelle 9</b><br>\n",
    "CyLinter laeuft automatisch durch alle Module von <b>aggregateData bis zum Ende</b><br>\n",
    "Module mit existierenden Checkpoints werden <b>automatisch uebersprungen</b><br><br>\n",
    "<b>Fuer TAG 1 (erste Analyse):</b> Waehlen Sie Marker, Speichern, Zelle 9 ausfuehren<br>\n",
    "<b>Fuer TAG 2+ (neue Marker):</b> Verwenden Sie Zelle 13 (Erweiterte Steuerung)\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Marker-Konfiguration</h3>\"),\n",
    "    info_box,\n",
    "    quick_label,\n",
    "    quick_option,\n",
    "    quick_info,\n",
    "    detail_label,\n",
    "    bio_controls,\n",
    "    bio_grid,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    save_btn,\n",
    "    status_output\n",
    "])\n",
    "\n",
    "display(ui)\n",
    "\n",
    "print(\"\\nKonfigurations-GUI bereit!\")\n",
    "print(\"   Waehlen Sie Marker-Strategie und klicken Sie 'Speichern & Bereit'\")\n",
    "print(\"   Pipeline wird dann in Zelle 9 gestartet.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CYLINTER PIPELINE-START (TAG 1)\n",
      "================================================================================\n",
      "Intelligente Checkpoint-Erkennung: Startet bei erstem fehlenden Modul!\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "CHECKPOINT-ANALYSE:\n",
      "======================================================================\n",
      "\n",
      "Existierende Checkpoints: 0/15\n",
      "\n",
      "Fehlende Checkpoints: 15/15\n",
      "   [--] aggregateData\n",
      "   [--] selectROIs\n",
      "   [--] intensityFilter\n",
      "   [--] areaFilter\n",
      "   [--] cycleCorrelation\n",
      "   [--] logTransform\n",
      "   [--] pruneOutliers\n",
      "   [--] metaQC\n",
      "   [--] PCA\n",
      "   [--] setContrast\n",
      "   [--] gating\n",
      "   [--] clustering\n",
      "   [--] clustermap\n",
      "   [--] frequencyStats\n",
      "   [--] curateThumbnails\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24e6fece4014356846cea4ae3cc917e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Pipeline-Start (Intelligente Checkpoint-Erkennung)</h3>'), HTML(value='\\n    <dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline-Start bereit!\n",
      "   Klicken Sie 'Pipeline starten' um bei 'aggregateData' zu beginnen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zelle 9: Pipeline-Start (TAG 1) - MIT INTELLIGENTER CHECKPOINT-ERKENNUNG\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CYLINTER PIPELINE-START (TAG 1)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Intelligente Checkpoint-Erkennung: Startet bei erstem fehlenden Modul!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ueberpruefe benoetigte Variablen\n",
    "if 'BASE_DIR' not in locals():\n",
    "    BASE_DIR = Path.cwd()\n",
    "if 'CONFIG_PATH' not in locals():\n",
    "    CONFIG_PATH = BASE_DIR / \"cylinter_config.yml\"\n",
    "if 'CYLINTER_ENV_PATH' not in locals():\n",
    "    CYLINTER_ENV_PATH = Path(\"/opt/anaconda3/envs/cylinter_ben\")\n",
    "if 'CHECKPOINT_DIR' not in locals():\n",
    "    CHECKPOINT_DIR = BASE_DIR / \"cylinter_output_prune_test\" / \"checkpoints\"\n",
    "\n",
    "# CyLinter Executables\n",
    "cylinter_executable = CYLINTER_ENV_PATH / \"bin\" / \"cylinter\"\n",
    "python_executable = CYLINTER_ENV_PATH / \"bin\" / \"python\"\n",
    "\n",
    "# Alle Module in Pipeline-Reihenfolge\n",
    "ALL_MODULES = [\n",
    "    \"aggregateData\", \"selectROIs\", \"intensityFilter\", \"areaFilter\",\n",
    "    \"cycleCorrelation\", \"logTransform\", \"pruneOutliers\", \"metaQC\", \"PCA\",\n",
    "    \"setContrast\", \"gating\", \"clustering\", \"clustermap\", \"frequencyStats\", \"curateThumbnails\"\n",
    "]\n",
    "\n",
    "# Output Widget\n",
    "output_widget = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ccc', padding='10px', margin='10px 0',\n",
    "    max_height='600px', overflow_y='auto'\n",
    "))\n",
    "\n",
    "# Pruefe ob Config existiert\n",
    "if not CONFIG_PATH.exists():\n",
    "    error_box = widgets.HTML(\"\"\"\n",
    "    <div style=\"background-color: #ffebee; padding: 20px; border-left: 4px solid #f44336;\">\n",
    "    <h3>FEHLER: Konfiguration fehlt!</h3>\n",
    "    <p>Die Datei <code>cylinter_config.yml</code> wurde nicht gefunden.</p>\n",
    "    <p><b>Bitte fuehren Sie zuerst ZELLE 8 aus!</b></p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    ui = widgets.VBox([error_box])\n",
    "    display(ui)\n",
    "    \n",
    "else:\n",
    "    # Config vorhanden - analysiere Checkpoints\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    markers_excluded = len(config.get('markersToExclude', []))\n",
    "    \n",
    "    # INTELLIGENTE CHECKPOINT-ERKENNUNG\n",
    "    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    existing_checkpoints = []\n",
    "    missing_checkpoints = []\n",
    "    \n",
    "    for module in ALL_MODULES:\n",
    "        checkpoint_file = CHECKPOINT_DIR / f\"{module}.parquet\"\n",
    "        if checkpoint_file.exists():\n",
    "            existing_checkpoints.append(module)\n",
    "        else:\n",
    "            missing_checkpoints.append(module)\n",
    "    \n",
    "    # Bestimme Startmodul\n",
    "    if len(missing_checkpoints) == 0:\n",
    "        suggested_start = None\n",
    "        status_message = \"ALLE CHECKPOINTS VORHANDEN! Pipeline komplett.\"\n",
    "        status_color = \"#4caf50\"\n",
    "    else:\n",
    "        suggested_start = missing_checkpoints[0]\n",
    "        status_message = f\"Erstes fehlendes Checkpoint: {suggested_start}\"\n",
    "        status_color = \"#ff9800\"\n",
    "    \n",
    "    # Zeige Status\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CHECKPOINT-ANALYSE:\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    print(f\"Existierende Checkpoints: {len(existing_checkpoints)}/15\")\n",
    "    for cp in existing_checkpoints:\n",
    "        print(f\"   [OK] {cp}\")\n",
    "    print(f\"\\nFehlende Checkpoints: {len(missing_checkpoints)}/15\")\n",
    "    for cp in missing_checkpoints:\n",
    "        print(f\"   [--] {cp}\")\n",
    "    print()\n",
    "    \n",
    "    # Pipeline-Start-Funktion\n",
    "    def start_pipeline(b):\n",
    "        \"\"\"Startet die Pipeline beim ersten fehlenden Modul\"\"\"\n",
    "        with output_widget:\n",
    "            output_widget.clear_output()\n",
    "            \n",
    "            # Re-check Checkpoints (falls sich was geaendert hat)\n",
    "            missing = []\n",
    "            for module in ALL_MODULES:\n",
    "                if not (CHECKPOINT_DIR / f\"{module}.parquet\").exists():\n",
    "                    missing.append(module)\n",
    "            \n",
    "            if len(missing) == 0:\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"ALLE CHECKPOINTS VORHANDEN!\")\n",
    "                print(\"=\"*70 + \"\\n\")\n",
    "                print(\"Die Pipeline ist komplett durchgelaufen.\")\n",
    "                print(\"Falls Sie Module neu berechnen wollen, nutzen Sie Zelle 12.\")\n",
    "                return\n",
    "            \n",
    "            start_module = missing[0]\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(f\"STARTE PIPELINE BEI: {start_module}\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "            print(\"Pipeline-Reihenfolge:\")\n",
    "            for i, mod in enumerate(ALL_MODULES, 1):\n",
    "                status = \"[SKIP]\" if mod in existing_checkpoints else \"[RUN!]\"\n",
    "                marker = \">>>\" if mod == start_module else \"   \"\n",
    "                print(f\"{marker} {i:2}. {status} {mod}\")\n",
    "            print()\n",
    "            \n",
    "            # WICHTIG: Wenn start_module NICHT aggregateData ist, aber auch nicht\n",
    "            # das vorherige Modul hat, muessen wir bei aggregateData starten!\n",
    "            if start_module != \"aggregateData\":\n",
    "                module_index = ALL_MODULES.index(start_module)\n",
    "                previous_module = ALL_MODULES[module_index - 1]\n",
    "                previous_checkpoint = CHECKPOINT_DIR / f\"{previous_module}.parquet\"\n",
    "                \n",
    "                if not previous_checkpoint.exists():\n",
    "                    print(f\"[!] WARNUNG: Vorheriges Checkpoint '{previous_module}.parquet' fehlt!\")\n",
    "                    print(f\"[!] Muss bei 'aggregateData' starten statt '{start_module}'!\\n\")\n",
    "                    start_module = \"aggregateData\"\n",
    "            \n",
    "            if cylinter_executable.exists():\n",
    "                cmd = [str(cylinter_executable), \"--module\", start_module, str(CONFIG_PATH)]\n",
    "                print(f\"[*] Command: cylinter --module {start_module} config.yml\")\n",
    "            else:\n",
    "                cmd = [str(python_executable), \"-m\", \"cylinter.cylinter\", \"--module\", start_module, str(CONFIG_PATH)]\n",
    "                print(f\"[*] Command: python -m cylinter.cylinter --module {start_module} config.yml\")\n",
    "            \n",
    "            print(f\"\\n[!] Pipeline laeuft von '{start_module}' bis zum Ende!\")\n",
    "            print(\"[!] Interaktive Module (selectROIs, setContrast, gating) oeffnen GUIs\\n\")\n",
    "            \n",
    "            overall_start = time.time()\n",
    "            \n",
    "            try:\n",
    "                env = os.environ.copy()\n",
    "                process = subprocess.Popen(\n",
    "                    cmd, cwd=str(BASE_DIR), env=env,\n",
    "                    stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                    universal_newlines=True, bufsize=1\n",
    "                )\n",
    "                \n",
    "                # Stream Output\n",
    "                for line in process.stdout:\n",
    "                    print(line, end='')\n",
    "                \n",
    "                return_code = process.wait()\n",
    "                total_duration = time.time() - overall_start\n",
    "                \n",
    "                # Zusammenfassung\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                if return_code == 0:\n",
    "                    print(\"PIPELINE ERFOLGREICH!\")\n",
    "                    print(f\"   Laufzeit: {total_duration:.1f}s ({total_duration/60:.1f} Min)\")\n",
    "                else:\n",
    "                    print(f\"PIPELINE FEHLER (Exit Code: {return_code})\")\n",
    "                    print(f\"   Laufzeit: {total_duration:.1f}s ({total_duration/60:.1f} Min)\")\n",
    "                print(\"=\"*70 + \"\\n\")\n",
    "                \n",
    "                # Zeige welche Checkpoints jetzt existieren\n",
    "                print(\"Checkpoint-Status nach Run:\")\n",
    "                for module in ALL_MODULES:\n",
    "                    cp_file = CHECKPOINT_DIR / f\"{module}.parquet\"\n",
    "                    status = \"[OK]\" if cp_file.exists() else \"[--]\"\n",
    "                    print(f\"   {status} {module}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\n[!] FEHLER beim Pipeline-Start: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Start Button\n",
    "    start_btn = widgets.Button(\n",
    "        description='Pipeline starten',\n",
    "        button_style='success' if suggested_start else 'primary',\n",
    "        layout=widgets.Layout(width='200px', height='50px'),\n",
    "        icon='play',\n",
    "        tooltip=f'Startet bei: {suggested_start}' if suggested_start else 'Pipeline komplett'\n",
    "    )\n",
    "    start_btn.on_click(start_pipeline)\n",
    "    \n",
    "    # Info Box\n",
    "    info_html = f\"\"\"\n",
    "    <div style=\"background-color: {status_color}22; padding: 15px; border-left: 4px solid {status_color};\">\n",
    "    <b>Status:</b> {status_message}<br><br>\n",
    "    <b>Konfiguration:</b><br>\n",
    "    {96 - markers_excluded} Marker werden verwendet<br><br>\n",
    "    <b>Existierende Checkpoints:</b> {len(existing_checkpoints)}/15<br>\n",
    "    <b>Fehlende Checkpoints:</b> {len(missing_checkpoints)}/15<br><br>\n",
    "    \"\"\"\n",
    "    \n",
    "    if suggested_start:\n",
    "        info_html += f\"\"\"\n",
    "        <b>Naechster Schritt:</b><br>\n",
    "        Pipeline wird bei <b>{suggested_start}</b> starten<br>\n",
    "        und bis zum Ende durchlaufen.<br><br>\n",
    "        \"\"\"\n",
    "    else:\n",
    "        info_html += \"\"\"\n",
    "        <b>Pipeline komplett!</b><br>\n",
    "        Alle Module wurden bereits ausgefuehrt.<br><br>\n",
    "        \"\"\"\n",
    "    \n",
    "    info_html += \"\"\"\n",
    "    <b>WICHTIG:</b><br>\n",
    "    CyLinter ueberschreibt alle Checkpoints ab dem Startmodul!<br>\n",
    "    Wenn Sie einzelne Module neu berechnen wollen, nutzen Sie Zelle 12.\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    info_box = widgets.HTML(info_html)\n",
    "    \n",
    "    # UI\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Pipeline-Start (Intelligente Checkpoint-Erkennung)</h3>\"),\n",
    "        info_box,\n",
    "        start_btn,\n",
    "        widgets.HTML(\"<hr><h4>Pipeline-Output:</h4>\"),\n",
    "        output_widget\n",
    "    ])\n",
    "    \n",
    "    display(ui)\n",
    "    \n",
    "    print(\"\\nPipeline-Start bereit!\")\n",
    "    if suggested_start:\n",
    "        print(f\"   Klicken Sie 'Pipeline starten' um bei '{suggested_start}' zu beginnen.\\n\")\n",
    "    else:\n",
    "        print(\"   Pipeline ist bereits komplett!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb3e456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"background-color: #fff3cd; padding: 30px; border: 4px solid #ff9800; text-align: center;\">\n",
       "<h1 style=\"color: #f57c00; margin: 0;\">STOP - TAG 1 ENDET HIER!</h1>\n",
       "<hr>\n",
       "<h3>Sie haben die GUIs fuer TAG 1 geladen:</h3>\n",
       "<p style=\"font-size: 18px;\">\n",
       "[+] Zelle 8: Marker-Konfiguration<br>\n",
       "[+] Zelle 9: Pipeline-Start\n",
       "</p>\n",
       "<hr>\n",
       "<h3>NAECHSTE SCHRITTE:</h3>\n",
       "<ol style=\"font-size: 16px; text-align: left; max-width: 600px; margin: auto;\">\n",
       "<li><b>Zelle 8:</b> Marker auswaehlen â†’ \"Speichern & Bereit\"</li>\n",
       "<li><b>Zelle 9:</b> \"Pipeline starten\" klicken</li>\n",
       "<li>Warten Sie bis Pipeline komplett durchgelaufen ist</li>\n",
       "</ol>\n",
       "<hr>\n",
       "<h3 style=\"color: #d32f2f;\">WARNUNG:</h3>\n",
       "<p style=\"font-size: 16px;\">\n",
       "Die Zellen UNTERHALB sind fuer TAG 2+ (Folge-Sessions)!<br>\n",
       "<b>Fuehren Sie diese NICHT aus waehrend TAG 1 laeuft!</b>\n",
       "</p>\n",
       "<hr>\n",
       "<p style=\"font-size: 14px; color: #666;\">\n",
       "Falls Sie \"Run All\" benutzt haben: Das ist okay!<br>\n",
       "Diese Zelle stoppt die automatische Ausfuehrung hier.\n",
       "</p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TAG 1 WORKFLOW BEREIT!\n",
      "================================================================================\n",
      "\n",
      "Gehen Sie zu:\n",
      "  1. Zelle 8: Marker konfigurieren\n",
      "  2. Zelle 9: Pipeline starten\n",
      "\n",
      "Die Zellen unterhalb (12-13) sind fuer TAG 2+ (spaetere Sessions).\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "STOP: TAG 1 Setup komplett. Verwenden Sie Zelle 8 und 9.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m STOP: TAG 1 Setup komplett. Verwenden Sie Zelle 8 und 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cylinter_ben/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# STOP-Zelle - TAG 1 ENDET HIER!\n",
    "\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Zeige STOP-Warnung\n",
    "stop_message = HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 30px; border: 4px solid #ff9800; text-align: center;\">\n",
    "<h1 style=\"color: #f57c00; margin: 0;\">STOP - TAG 1 ENDET HIER!</h1>\n",
    "<hr>\n",
    "<h3>Sie haben die GUIs fuer TAG 1 geladen:</h3>\n",
    "<p style=\"font-size: 18px;\">\n",
    "[+] Zelle 8: Marker-Konfiguration<br>\n",
    "[+] Zelle 9: Pipeline-Start\n",
    "</p>\n",
    "<hr>\n",
    "<h3>NAECHSTE SCHRITTE:</h3>\n",
    "<ol style=\"font-size: 16px; text-align: left; max-width: 600px; margin: auto;\">\n",
    "<li><b>Zelle 8:</b> Marker auswaehlen â†’ \"Speichern & Bereit\"</li>\n",
    "<li><b>Zelle 9:</b> \"Pipeline starten\" klicken</li>\n",
    "<li>Warten Sie bis Pipeline komplett durchgelaufen ist</li>\n",
    "</ol>\n",
    "<hr>\n",
    "<h3 style=\"color: #d32f2f;\">WARNUNG:</h3>\n",
    "<p style=\"font-size: 16px;\">\n",
    "Die Zellen UNTERHALB sind fuer TAG 2+ (Folge-Sessions)!<br>\n",
    "<b>Fuehren Sie diese NICHT aus waehrend TAG 1 laeuft!</b>\n",
    "</p>\n",
    "<hr>\n",
    "<p style=\"font-size: 14px; color: #666;\">\n",
    "Falls Sie \"Run All\" benutzt haben: Das ist okay!<br>\n",
    "Diese Zelle stoppt die automatische Ausfuehrung hier.\n",
    "</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "display(stop_message)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TAG 1 WORKFLOW BEREIT!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGehen Sie zu:\")\n",
    "print(\"  1. Zelle 8: Marker konfigurieren\")\n",
    "print(\"  2. Zelle 9: Pipeline starten\")\n",
    "print(\"\\nDie Zellen unterhalb (12-13) sind fuer TAG 2+ (spaetere Sessions).\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# WICHTIG: Verhindere weiteres \"Run All\"\n",
    "# Kommentieren Sie diese Zeile aus, wenn Sie Zelle 12 ausfuehren wollen:\n",
    "raise SystemExit(\"STOP: TAG 1 Setup komplett. Verwenden Sie Zelle 8 und 9.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558c408",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ?? Zelle 12: Erweiterte Pipeline-Steuerung (TAG 2+ oder Fehlerkorrektur)\n",
    "\n",
    "**?? NUR VERWENDEN WENN:**\n",
    "\n",
    "**Szenario A - Neue Marker hinzuf?gen (TAG 2+):**\n",
    "- Sie haben TAG 1 komplett abgeschlossen\n",
    "- Sie wollen zus?tzliche Marker hinzuf?gen\n",
    "- Alte Gating-Ergebnisse sollen erhalten bleiben\n",
    "\n",
    "**Szenario B - Modul neu starten (Fehlerkorrektur):**\n",
    "- Ein Modul ist fehlgeschlagen oder Ergebnis ist nicht korrekt\n",
    "- Sie wollen nur dieses Modul (und nachfolgende) neu laufen lassen\n",
    "- Vorherige Module sollen NICHT neu berechnet werden\n",
    "\n",
    "---\n",
    "\n",
    "**Diese GUI bietet:**\n",
    "- ? **Neue Marker extrahieren & mergen** (wie Zelle 12 vorher)\n",
    "- ? **Vollst?ndige Pipeline-Modul-Kontrolle** (wie Zelle 8)\n",
    "- ? **Checkpoint-Reset f?r einzelne Module** (z.B. nur `gating` neu starten)\n",
    "\n",
    "---\n",
    "\n",
    "**Workflow TAG 2+ (Neue Marker):**\n",
    "```\n",
    "1. Kernel ? Restart\n",
    "2. Nur Zellen 2-3 ausf?hren\n",
    "3. Diese Zelle ausf?hren ? GUI ?ffnet sich\n",
    "4. Tab 1: Neue Marker ausw?hlen ? \"Extract & Merge\"\n",
    "5. Tab 2: Nur \"aggregateData\" + \"gating\" ausw?hlen ? \"Run Selected\"\n",
    "```\n",
    "\n",
    "**Workflow Fehlerkorrektur (Modul neu starten):**\n",
    "```\n",
    "1. Kernel ? Restart\n",
    "2. Nur Zellen 2-3 ausf?hren\n",
    "3. Diese Zelle ausf?hren ? GUI ?ffnet sich\n",
    "4. Tab 2: \"Reset Checkpoint\" f?r fehlerhaftes Modul klicken\n",
    "5. Tab 2: Fehlerhaftes Modul + nachfolgende ausw?hlen ? \"Run Selected\"\n",
    "```\n",
    "\n",
    "**?? Bei ERSTER SESSION (TAG 1): Verwenden Sie Zelle 8 + Zelle 10!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f306b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ERWEITERTE PIPELINE-STEUERUNG (TAG 2+ / FEHLERKORREKTUR)\n",
      "================================================================================\n",
      "Mit intelligenter Checkpoint-Erkennung!\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e490dfbc185d49b48f9974c3c9522601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Erweiterte Pipeline-Steuerung</h2>'), HTML(value='\\n<div style=\"background-coloâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ERWEITERTE PIPELINE-STEUERUNGS-GUI BEREIT!\n",
      "   [*] Tab 1: 39 Bio-Marker verfuegbar\n",
      "   [*] Tab 2: 15 Pipeline-Module\n",
      "   [*] Tab 2: Checkpoints: 10 OK, 5 fehlend\n",
      "   [*] Tab 2: Vorgeschlagener Start: gating\n",
      "   [*] Tab 2: INTELLIGENTE Validierung aktiviert!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zelle 12: Erweiterte Pipeline-Steuerung (TAG 2+ / Fehlerkorrektur) - MIT INTELLIGENTER CHECKPOINT-ERKENNUNG\n",
    "\n",
    "# IMPORTS FIRST!\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from skimage.measure import regionprops_table\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ERWEITERTE PIPELINE-STEUERUNG (TAG 2+ / FEHLERKORREKTUR)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Mit intelligenter Checkpoint-Erkennung!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ueberpruefe benoetigte Variablen\n",
    "if 'BASE_DIR' not in locals():\n",
    "    BASE_DIR = Path.cwd()\n",
    "if 'CONFIG_PATH' not in locals():\n",
    "    CONFIG_PATH = BASE_DIR / \"cylinter_config.yml\"\n",
    "if 'CYLINTER_ENV_PATH' not in locals():\n",
    "    CYLINTER_ENV_PATH = Path(\"/opt/anaconda3/envs/cylinter_ben\")\n",
    "if 'CHECKPOINT_DIR' not in locals():\n",
    "    CHECKPOINT_DIR = BASE_DIR / \"cylinter_output_prune_test\" / \"checkpoints\"\n",
    "if 'MARKERS_CSV_PATH' not in locals():\n",
    "    MARKERS_CSV_PATH = BASE_DIR / \"markers.csv\"\n",
    "\n",
    "# CyLinter Executables\n",
    "cylinter_executable = CYLINTER_ENV_PATH / \"bin\" / \"cylinter\"\n",
    "python_executable = CYLINTER_ENV_PATH / \"bin\" / \"python\"\n",
    "\n",
    "# Alle Module (Pipeline-Reihenfolge)\n",
    "ALL_MODULES = [\n",
    "    \"aggregateData\", \"selectROIs\", \"intensityFilter\", \"areaFilter\",\n",
    "    \"cycleCorrelation\", \"logTransform\", \"pruneOutliers\", \"metaQC\", \"PCA\",\n",
    "    \"setContrast\", \"gating\", \"clustering\", \"clustermap\", \"frequencyStats\", \"curateThumbnails\"\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# TAB 1: NEUE MARKER EXTRAHIEREN & MERGEN (TAG 2)\n",
    "# ============================================================================\n",
    "\n",
    "# Lade markers.csv\n",
    "df_markers = pd.read_csv(MARKERS_CSV_PATH)\n",
    "bio_markers = df_markers[\n",
    "    ~df_markers['marker_name'].str.contains('DAPI', case=False, na=False) &\n",
    "    ~df_markers['marker_name'].str.contains('AF[12]_', case=True, na=False, regex=True)\n",
    "]['marker_name'].tolist()\n",
    "\n",
    "# Output fuer Marker-Extraktion (KEINE Hoehenbegrenzung!)\n",
    "marker_output = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ddd', padding='10px', margin='10px 0'\n",
    "))\n",
    "\n",
    "# Checkboxen fuer Marker\n",
    "bio_checkboxes = {}\n",
    "bio_checkbox_widgets = []\n",
    "for marker in sorted(bio_markers):\n",
    "    cb = widgets.Checkbox(value=False, description=marker, indent=False,\n",
    "                          layout=widgets.Layout(width='220px'))\n",
    "    bio_checkboxes[marker] = cb\n",
    "    bio_checkbox_widgets.append(cb)\n",
    "\n",
    "bio_grid = widgets.GridBox(bio_checkbox_widgets, layout=widgets.Layout(\n",
    "    grid_template_columns='repeat(3, 220px)', grid_gap='5px',\n",
    "    margin='10px 0'\n",
    "))\n",
    "\n",
    "# Buttons fuer Marker-Tab\n",
    "marker_select_all = widgets.Button(description='[*] Select All', layout=widgets.Layout(width='150px'))\n",
    "marker_deselect_all = widgets.Button(description='[ ] Deselect All', layout=widgets.Layout(width='150px'))\n",
    "\n",
    "def marker_select_all_click(_):\n",
    "    for cb in bio_checkboxes.values():\n",
    "        cb.value = True\n",
    "\n",
    "def marker_deselect_all_click(_):\n",
    "    for cb in bio_checkboxes.values():\n",
    "        cb.value = False\n",
    "\n",
    "marker_select_all.on_click(marker_select_all_click)\n",
    "marker_deselect_all.on_click(marker_deselect_all_click)\n",
    "\n",
    "marker_controls = widgets.HBox([marker_select_all, marker_deselect_all])\n",
    "\n",
    "# Extract & Merge Button\n",
    "extract_btn = widgets.Button(\n",
    "    description='[>] Extract & Merge New Markers',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='280px', height='45px'),\n",
    "    icon='flask'\n",
    ")\n",
    "\n",
    "def extract_and_merge_markers(b):\n",
    "    \"\"\"Extrahiert neue Marker und merged sie mit bestehender CSV\"\"\"\n",
    "    with marker_output:\n",
    "        marker_output.clear_output()\n",
    "        \n",
    "        selected_markers = [marker for marker, cb in bio_checkboxes.items() if cb.value]\n",
    "        \n",
    "        if not selected_markers:\n",
    "            print(\"WARNUNG: KEINE MARKER AUSGEWAEHLT!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MARKER-EXTRAKTION & MERGE\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        print(f\"Ausgewaehlte Marker: {len(selected_markers)}\")\n",
    "        \n",
    "        try:\n",
    "            # Lade Config\n",
    "            with open(CONFIG_PATH, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            \n",
    "            inDir = BASE_DIR / config.get('inDir', '.')\n",
    "            csv_dir = config.get('csv_dir', 'csv')\n",
    "            smd = config['sampleMetadata']\n",
    "            sample_name = list(smd.values())[0][0]\n",
    "            \n",
    "            csv_path = inDir / csv_dir / f\"{sample_name}.csv\"\n",
    "            tif_path = inDir / 'tif' / f\"{sample_name}.ome.tif\"\n",
    "            mask_path = inDir / 'mask' / f\"{sample_name}.tif\"\n",
    "            \n",
    "            print(f\"[+] Sample: {sample_name}\")\n",
    "            \n",
    "            # Lade alte CSV\n",
    "            if not csv_path.exists():\n",
    "                print(f\"[!] CSV nicht gefunden: {csv_path}\")\n",
    "                return\n",
    "            \n",
    "            old_csv = pd.read_csv(csv_path)\n",
    "            print(f\"[+] Alte CSV geladen: {len(old_csv.columns)} Spalten\")\n",
    "            \n",
    "            # Extrahiere neue Marker aus TIF\n",
    "            print(f\"\\n[*] Extrahiere {len(selected_markers)} neue Marker...\")\n",
    "            \n",
    "            tif_data = tifffile.imread(tif_path)\n",
    "            mask_data = tifffile.imread(mask_path)\n",
    "            \n",
    "            marker_indices = [i for i, m in enumerate(df_markers['marker_name']) if m in selected_markers]\n",
    "            \n",
    "            print(f\"[+] TIF Shape: {tif_data.shape}\")\n",
    "            print(f\"[+] Mask Shape: {mask_data.shape}\")\n",
    "            \n",
    "            new_data = {}\n",
    "            for idx, marker in zip(marker_indices, selected_markers):\n",
    "                channel_data = tif_data[idx]\n",
    "                props = regionprops_table(mask_data, intensity_image=channel_data,\n",
    "                                         properties=['label', 'mean_intensity'])\n",
    "                new_data[marker] = dict(zip(props['label'], props['mean_intensity']))\n",
    "                print(f\"  [+] {marker}: {len(props['label'])} Zellen\")\n",
    "            \n",
    "            # Merge mit alter CSV\n",
    "            print(f\"\\n[*] Merge mit bestehender CSV...\")\n",
    "            \n",
    "            new_columns_df = pd.DataFrame(new_data)\n",
    "            new_columns_df.index = old_csv.index\n",
    "            \n",
    "            merged_csv = pd.concat([old_csv, new_columns_df], axis=1)\n",
    "            \n",
    "            print(f\"[+] Alte CSV: {len(old_csv.columns)} Spalten\")\n",
    "            print(f\"[+] Neue Spalten: {len(selected_markers)}\")\n",
    "            print(f\"[+] Merged CSV: {len(merged_csv.columns)} Spalten\")\n",
    "            \n",
    "            # Backup & Save\n",
    "            from datetime import datetime\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            backup_path = csv_path.with_suffix(f'.csv.backup_{timestamp}')\n",
    "            \n",
    "            import shutil\n",
    "            shutil.copy2(csv_path, backup_path)\n",
    "            print(f\"[+] Backup: {backup_path.name}\")\n",
    "            \n",
    "            merged_csv.to_csv(csv_path, index=False)\n",
    "            print(f\"[+] CSV gespeichert: {csv_path.name}\")\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"ERFOLGREICH!\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            print(f\"[>] NAECHSTER SCHRITT:\")\n",
    "            print(f\"   1. Tab 2: Loesche aggregateData.parquet\")\n",
    "            print(f\"   2. Tab 2: Starte Pipeline bei aggregateData\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[!] FEHLER: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "extract_btn.on_click(extract_and_merge_markers)\n",
    "\n",
    "# Tab 1 Layout mit Scrollbar\n",
    "tab1_title = widgets.HTML(\"<h3>Neue Marker hinzufuegen (TAG 2+)</h3>\")\n",
    "tab1_info = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107;\">\n",
    "<b>Zweck:</b> Zusaetzliche Marker zur bestehenden CSV hinzufuegen<br><br>\n",
    "<b>Workflow:</b><br>\n",
    "1. Waehlen Sie neue Marker aus<br>\n",
    "2. Klicken Sie \"Extract & Merge\"<br>\n",
    "3. Gehen Sie zu Tab 2 â†’ Loeschen Sie aggregateData Checkpoint<br>\n",
    "4. Starten Sie Pipeline bei aggregateData\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "tab1_content = widgets.VBox([\n",
    "    tab1_title,\n",
    "    tab1_info,\n",
    "    marker_controls,\n",
    "    bio_grid,\n",
    "    extract_btn,\n",
    "    marker_output\n",
    "])  # KEINE Hoehenbegrenzung mehr!\n",
    "\n",
    "# ============================================================================\n",
    "# TAB 2: CHECKPOINT-RESET & INTELLIGENTE PIPELINE-START\n",
    "# ============================================================================\n",
    "\n",
    "# Output fuer Pipeline (KEINE Hoehenbegrenzung!)\n",
    "pipeline_output = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ccc', padding='10px', margin='10px 0'\n",
    "))\n",
    "\n",
    "# Checkpoint Reset Buttons (fuer jedes Modul)\n",
    "reset_buttons = {}\n",
    "reset_button_widgets = []\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Report Path fuer Gating-Reset\n",
    "REPORT_PATH = BASE_DIR / \"cylinter_output_prune_test\" / \"cylinter_report.yml\"\n",
    "\n",
    "for module in ALL_MODULES:\n",
    "    btn = widgets.Button(\n",
    "        description=f'[X] {module}',\n",
    "        button_style='danger',\n",
    "        layout=widgets.Layout(width='220px'),\n",
    "        tooltip=f'Loescht Checkpoint fuer {module}' if module != 'gating' else 'Loescht Checkpoint UND Report-Daten fuer Gating'\n",
    "    )\n",
    "    \n",
    "    def make_reset_handler(mod):\n",
    "        def handler(_):\n",
    "            with pipeline_output:\n",
    "                checkpoint_file = CHECKPOINT_DIR / f\"{mod}.parquet\"\n",
    "                \n",
    "                # Loesche Checkpoint\n",
    "                if checkpoint_file.exists():\n",
    "                    checkpoint_file.unlink()\n",
    "                    print(f\"[+] Checkpoint geloescht: {mod}.parquet\")\n",
    "                    print(f\"  -> Modul '{mod}' wird beim naechsten Run neu berechnet\")\n",
    "                else:\n",
    "                    print(f\"[i] Kein Checkpoint vorhanden: {mod}.parquet\")\n",
    "                \n",
    "                # SPEZIAL: Fuer Gating auch Report-Daten loeschen!\n",
    "                if mod == 'gating' and REPORT_PATH.exists():\n",
    "                    try:\n",
    "                        import yaml\n",
    "                        with open(REPORT_PATH, 'r') as f:\n",
    "                            report = yaml.safe_load(f)\n",
    "                        \n",
    "                        if 'gating' in report and report['gating']:\n",
    "                            num_gates = len(report['gating'])\n",
    "                            report['gating'] = {}\n",
    "                            \n",
    "                            with open(REPORT_PATH, 'w') as f:\n",
    "                                yaml.dump(report, f, default_flow_style=False)\n",
    "                            \n",
    "                            print(f\"[+] {num_gates} Gating-Eintraege aus Report geloescht\")\n",
    "                            print(f\"  -> Gating-GUI wird sich beim naechsten Run oeffnen!\")\n",
    "                        else:\n",
    "                            print(f\"[i] Keine Gating-Daten im Report vorhanden\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[!] Fehler beim Loeschen der Report-Daten: {e}\")\n",
    "                \n",
    "                print()\n",
    "        return handler\n",
    "    \n",
    "    btn.on_click(make_reset_handler(module))\n",
    "    reset_buttons[module] = btn\n",
    "    reset_button_widgets.append(btn)\n",
    "\n",
    "reset_grid = widgets.GridBox(reset_button_widgets, layout=widgets.Layout(\n",
    "    grid_template_columns='repeat(3, 220px)',\n",
    "    grid_gap='10px',\n",
    "    margin='10px 0'\n",
    "))\n",
    "\n",
    "# INTELLIGENTE CHECKPOINT-ERKENNUNG\n",
    "existing_checkpoints = []\n",
    "missing_checkpoints = []\n",
    "\n",
    "for module in ALL_MODULES:\n",
    "    checkpoint_file = CHECKPOINT_DIR / f\"{module}.parquet\"\n",
    "    if checkpoint_file.exists():\n",
    "        existing_checkpoints.append(module)\n",
    "    else:\n",
    "        missing_checkpoints.append(module)\n",
    "\n",
    "# Bestimme Vorschlag\n",
    "if len(missing_checkpoints) == 0:\n",
    "    suggested_start = ALL_MODULES[0]  # aggregateData als Default\n",
    "else:\n",
    "    suggested_start = missing_checkpoints[0]\n",
    "\n",
    "# Dropdown fuer Modul-Start (mit Vorschlag)\n",
    "module_dropdown = widgets.Dropdown(\n",
    "    options=ALL_MODULES,\n",
    "    value=suggested_start,\n",
    "    description='Start bei:',\n",
    "    layout=widgets.Layout(width='300px'),\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "# Pipeline Start Button\n",
    "run_from_module_btn = widgets.Button(\n",
    "    description='[>] Pipeline starten',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px', height='40px'),\n",
    "    icon='play',\n",
    "    tooltip='Startet Pipeline ab gewaehltem Modul'\n",
    ")\n",
    "\n",
    "def run_from_module(b):\n",
    "    \"\"\"Startet Pipeline ab gewaehltem Modul - MIT INTELLIGENTER CHECKPOINT-VALIDIERUNG\"\"\"\n",
    "    with pipeline_output:\n",
    "        pipeline_output.clear_output()\n",
    "        \n",
    "        start_module = module_dropdown.value\n",
    "        \n",
    "        # Re-check Checkpoints\n",
    "        missing = []\n",
    "        for module in ALL_MODULES:\n",
    "            if not (CHECKPOINT_DIR / f\"{module}.parquet\").exists():\n",
    "                missing.append(module)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"STARTE PIPELINE AB '{start_module}'\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        print(\"Checkpoint-Status:\")\n",
    "        for i, mod in enumerate(ALL_MODULES, 1):\n",
    "            status = \"[OK]\" if mod not in missing else \"[--]\"\n",
    "            marker = \">>>\" if mod == start_module else \"   \"\n",
    "            print(f\"{marker} {i:2}. {status} {mod}\")\n",
    "        print()\n",
    "        \n",
    "        # INTELLIGENTE VALIDIERUNG: Finde tatsaechliches Startmodul\n",
    "        actual_start_module = start_module\n",
    "        \n",
    "        if start_module != \"aggregateData\":\n",
    "            module_index = ALL_MODULES.index(start_module)\n",
    "            previous_module = ALL_MODULES[module_index - 1]\n",
    "            previous_checkpoint = CHECKPOINT_DIR / f\"{previous_module}.parquet\"\n",
    "            \n",
    "            if not previous_checkpoint.exists():\n",
    "                print(f\"[!] WARNUNG: Vorheriges Checkpoint '{previous_module}.parquet' fehlt!\")\n",
    "                print(f\"[!] CyLinter kann nicht bei '{start_module}' starten ohne vorheriges Checkpoint.\")\n",
    "                print(f\"[!] Suche nach letztem existierenden Checkpoint...\\n\")\n",
    "                \n",
    "                # Finde letztes existierendes Checkpoint VOR dem gewaehlten Modul\n",
    "                found_checkpoint = None\n",
    "                for i in range(module_index - 1, -1, -1):\n",
    "                    check_module = ALL_MODULES[i]\n",
    "                    check_file = CHECKPOINT_DIR / f\"{check_module}.parquet\"\n",
    "                    if check_file.exists():\n",
    "                        found_checkpoint = ALL_MODULES[i + 1]  # Naechstes Modul nach letztem Checkpoint\n",
    "                        break\n",
    "                \n",
    "                if found_checkpoint:\n",
    "                    actual_start_module = found_checkpoint\n",
    "                    print(f\"[+] Letztes Checkpoint gefunden: {ALL_MODULES[i]}.parquet\")\n",
    "                    print(f\"[+] Starte stattdessen bei: {actual_start_module}\")\n",
    "                else:\n",
    "                    actual_start_module = \"aggregateData\"\n",
    "                    print(f\"[!] Keine vorherigen Checkpoints gefunden!\")\n",
    "                    print(f\"[+] Starte bei: aggregateData (komplette Pipeline)\")\n",
    "                \n",
    "                print(f\"\\n[i] Pipeline laeuft von '{actual_start_module}' bis zum Ende\")\n",
    "                print(f\"[i] '{start_module}' wird dabei erreicht und neu berechnet.\\n\")\n",
    "        \n",
    "        # BUGFIX: Verwende actual_start_module statt start_module!\n",
    "        if cylinter_executable.exists():\n",
    "            cmd = [str(cylinter_executable), \"--module\", actual_start_module, str(CONFIG_PATH)]\n",
    "            print(f\"[*] Command: cylinter --module {actual_start_module} config.yml\")\n",
    "        else:\n",
    "            cmd = [str(python_executable), \"-m\", \"cylinter.cylinter\", \"--module\", actual_start_module, str(CONFIG_PATH)]\n",
    "            print(f\"[*] Command: python -m cylinter.cylinter --module {actual_start_module} config.yml\")\n",
    "        \n",
    "        print(f\"\\n[!] Pipeline laeuft von '{actual_start_module}' bis zum Ende!\")\n",
    "        print(f\"[!] CyLinter ueberschreibt alle Checkpoints ab '{actual_start_module}'!\\n\")\n",
    "        \n",
    "        overall_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            env = os.environ.copy()\n",
    "            process = subprocess.Popen(\n",
    "                cmd, cwd=str(BASE_DIR), env=env,\n",
    "                stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True, bufsize=1\n",
    "            )\n",
    "            \n",
    "            # Stream Output\n",
    "            for line in process.stdout:\n",
    "                print(line, end='')\n",
    "            \n",
    "            return_code = process.wait()\n",
    "            total_duration = time.time() - overall_start\n",
    "            \n",
    "            # Zusammenfassung\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            if return_code == 0:\n",
    "                print(f\"PIPELINE ERFOLGREICH!\")\n",
    "                print(f\"   Laufzeit: {total_duration:.1f}s ({total_duration/60:.1f} Min)\")\n",
    "            else:\n",
    "                print(f\"PIPELINE FEHLER (Exit Code: {return_code})\")\n",
    "                print(f\"   Laufzeit: {total_duration:.1f}s ({total_duration/60:.1f} Min)\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            \n",
    "            # Zeige neue Checkpoint-Status\n",
    "            print(\"Checkpoint-Status nach Run:\")\n",
    "            for module in ALL_MODULES:\n",
    "                cp_file = CHECKPOINT_DIR / f\"{module}.parquet\"\n",
    "                status = \"[OK]\" if cp_file.exists() else \"[--]\"\n",
    "                print(f\"   {status} {module}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[!] FEHLER: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "run_from_module_btn.on_click(run_from_module)\n",
    "\n",
    "# Tab 2 Layout mit Checkpoint-Info und Scrollbar\n",
    "checkpoint_info_html = f\"\"\"\n",
    "<div style=\"background-color: #e7f3ff; padding: 10px; border-left: 4px solid #2196F3; margin-bottom: 10px;\">\n",
    "<b>Checkpoint-Status:</b><br>\n",
    "Existierend: {len(existing_checkpoints)}/15<br>\n",
    "Fehlend: {len(missing_checkpoints)}/15<br><br>\n",
    "<b>Vorgeschlagener Start:</b> <code>{suggested_start}</code><br><br>\n",
    "<b>Hinweis:</b> CyLinter ueberschreibt alle Checkpoints ab dem Startmodul!\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "tab2_title = widgets.HTML(\"<h3>Checkpoint-Reset & Pipeline-Start</h3>\")\n",
    "tab2_checkpoint_info = widgets.HTML(checkpoint_info_html)\n",
    "tab2_info = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107;\">\n",
    "<b>Funktionen:</b><br>\n",
    "â€¢ <b>Checkpoint loeschen:</b> Klicken Sie auf Modul-Button oben<br>\n",
    "â€¢ <b>Pipeline starten:</b> Waehlen Sie Startmodul im Dropdown â†’ \"Pipeline starten\"<br><br>\n",
    "<b>WICHTIG:</b><br>\n",
    "â€¢ Pipeline laeuft vom Startmodul bis zum Ende<br>\n",
    "â€¢ Vorheriges Checkpoint muss existieren (ausser bei aggregateData)<br>\n",
    "â€¢ INTELLIGENT: Wenn Checkpoint fehlt, startet bei letztem vorhandenen Checkpoint<br>\n",
    "â€¢ Alle Checkpoints ab Startmodul werden ueberschrieben\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "tab2_content = widgets.VBox([\n",
    "    tab2_title,\n",
    "    tab2_checkpoint_info,\n",
    "    widgets.HTML(\"<h4>Checkpoint-Reset:</h4>\"),\n",
    "    reset_grid,\n",
    "    widgets.HTML(\"<hr><h4>Pipeline starten:</h4>\"),\n",
    "    tab2_info,\n",
    "    widgets.HBox([module_dropdown, run_from_module_btn]),\n",
    "    widgets.HTML(\"<hr><h4>Pipeline-Output:</h4>\"),\n",
    "    pipeline_output\n",
    "])  # KEINE Hoehenbegrenzung mehr!\n",
    "\n",
    "# ============================================================================\n",
    "# TABS KOMBINIEREN MIT SCROLLBAR\n",
    "# ============================================================================\n",
    "\n",
    "tabs = widgets.Tab(children=[tab1_content, tab2_content])\n",
    "tabs.set_title(0, 'Neue Marker (TAG 2+)')\n",
    "tabs.set_title(1, 'Checkpoint & Pipeline')\n",
    "\n",
    "main_title = widgets.HTML(\"<h2>Erweiterte Pipeline-Steuerung</h2>\")\n",
    "main_info = widgets.HTML(\"\"\"\n",
    "<div style=\"background-color: #fff3cd; padding: 15px; border-left: 4px solid #ffc107;\">\n",
    "<b>Diese GUI ist fuer fortgeschrittene Workflows:</b><br><br>\n",
    "<b>Use Cases:</b><br>\n",
    "â€¢ <b>TAG 2+:</b> Neue Marker hinzufuegen (Tab 1 â†’ Tab 2)<br>\n",
    "â€¢ <b>Fehlerkorrektur:</b> Module neu berechnen (Tab 2: Checkpoint loeschen â†’ Pipeline starten)<br>\n",
    "â€¢ <b>Ab Modul starten:</b> Pipeline ab bestimmtem Punkt fortsetzen (Tab 2)<br><br>\n",
    "<b>INTELLIGENT:</b> Wenn vorheriges Checkpoint fehlt, startet automatisch bei letztem vorhandenen!<br><br>\n",
    "<b>Fuer TAG 1 (erste Analyse):</b> Verwenden Sie Zelle 8 + Zelle 9!\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Main Container - KEINE Hoehenbegrenzung!\n",
    "ui = widgets.VBox([\n",
    "    main_title, \n",
    "    main_info, \n",
    "    tabs\n",
    "])\n",
    "\n",
    "# Display\n",
    "display(ui)\n",
    "\n",
    "print(\"\\nERWEITERTE PIPELINE-STEUERUNGS-GUI BEREIT!\")\n",
    "print(f\"   [*] Tab 1: {len(bio_markers)} Bio-Marker verfuegbar\")\n",
    "print(f\"   [*] Tab 2: {len(ALL_MODULES)} Pipeline-Module\")\n",
    "print(f\"   [*] Tab 2: Checkpoints: {len(existing_checkpoints)} OK, {len(missing_checkpoints)} fehlend\")\n",
    "print(f\"   [*] Tab 2: Vorgeschlagener Start: {suggested_start}\")\n",
    "print(f\"   [*] Tab 2: INTELLIGENTE Validierung aktiviert!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac34a2",
   "metadata": {},
   "source": [
    "## ?? Parquet Viewer (optional)\n",
    "\n",
    "Optionales Werkzeug, um Checkpoint-Parquet-Dateien einzusehen. L?dt keine Pipeline; vorher Zelle 11 ausf?hren, damit `CHECKPOINT_DIR` gesetzt ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3be7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 11 parquet file(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a82b88c0794590945983aa892068e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding:â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PARQUET DATA VIEWER - Kopieren Sie diesen gesamten Code in eine neue Notebook-Zelle\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "checkpoint_dir = CHECKPOINT_DIR\n",
    "parquet_files = sorted(checkpoint_dir.glob('*.parquet'))\n",
    "parquet_names = [f.name for f in parquet_files]\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"âŒ No parquet files found!\")\n",
    "else:\n",
    "    print(f\"âœ… Found {len(parquet_files)} parquet file(s)\")\n",
    "\n",
    "current_data = None\n",
    "\n",
    "title_html = widgets.HTML(value=\"\"\"<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px;'><h2 style='color: white; margin: 0;'>ðŸ“Š Parquet Viewer</h2></div>\"\"\")\n",
    "\n",
    "file_dropdown = widgets.Dropdown(options=parquet_names, value=parquet_names[-1] if parquet_names else None, description='File:', layout=widgets.Layout(width='500px'))\n",
    "load_btn = widgets.Button(description='ðŸ“‚ Load', button_style='primary')\n",
    "info_output = widgets.Output()\n",
    "preview_output = widgets.Output(layout=widgets.Layout(max_height='400px', overflow_y='auto'))\n",
    "export_filename = widgets.Text(value='data.csv', description='Name:')\n",
    "export_btn = widgets.Button(description='ðŸ’¾ Export CSV', button_style='success')\n",
    "\n",
    "def load_data(b):\n",
    "    global current_data\n",
    "    with info_output:\n",
    "        info_output.clear_output()\n",
    "        try:\n",
    "            current_data = pd.read_parquet(checkpoint_dir / file_dropdown.value)\n",
    "            print(f\"âœ… Loaded {len(current_data):,} rows, {len(current_data.columns)} cols\")\n",
    "            with preview_output:\n",
    "                preview_output.clear_output()\n",
    "                display(current_data.head(10))\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "\n",
    "def export_data(b):\n",
    "    with info_output:\n",
    "        info_output.clear_output()\n",
    "        if current_data is not None:\n",
    "            path = Path.cwd() / export_filename.value\n",
    "            current_data.to_csv(path, index=False)\n",
    "            print(f\"âœ… Exported to {path}\")\n",
    "        else:\n",
    "            print(\"âŒ Load data first!\")\n",
    "\n",
    "load_btn.on_click(load_data)\n",
    "export_btn.on_click(export_data)\n",
    "\n",
    "ui = widgets.VBox([title_html, widgets.HBox([file_dropdown, load_btn]), info_output, preview_output, widgets.HBox([export_filename, export_btn])], layout=widgets.Layout(padding='20px'))\n",
    "display(ui)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cylinter_ben",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

